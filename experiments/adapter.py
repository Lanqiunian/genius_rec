#!/usr/bin/env python3
"""
GENIUS-Rec å®éªŒé€‚é…è„šæœ¬
=====================

æ­¤è„šæœ¬ä¸“é—¨ç”¨äºé€‚é…å½“å‰çš„å¤šä¸“å®¶è§£ç å™¨æ¶æ„ï¼Œç¡®ä¿å®éªŒé…ç½®ä¸ä»£ç çŠ¶æ€ä¸€è‡´ã€‚

ä¸»è¦åŠŸèƒ½ï¼š
1. éªŒè¯å½“å‰è§£ç å™¨çŠ¶æ€
2. æ£€æŸ¥ä¸“å®¶ç³»ç»Ÿé…ç½®
3. è¿è¡Œé€‚é…åçš„å®éªŒ
4. ç”Ÿæˆå…¼å®¹æ€§æŠ¥å‘Š
"""

import sys
import torch
import logging
from pathlib import Path
import subprocess
import json
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°path
sys.path.append(str(Path(__file__).parent.parent))

from src.config import get_config
from src.decoder.decoder import GenerativeDecoder

class ExperimentAdapter:
    """å®éªŒé€‚é…å™¨ - ç¡®ä¿å®éªŒä¸å½“å‰ä»£ç çŠ¶æ€å…¼å®¹"""
    
    def __init__(self):
        self.config = get_config()
        self.base_dir = Path("/root/autodl-tmp/genius_rec-main")
        self.adapter_log = self.base_dir / "logs" / f"adapter_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
        
        # è®¾ç½®æ—¥å¿—
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(self.adapter_log),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
        
    def verify_decoder_compatibility(self):
        """éªŒè¯è§£ç å™¨å…¼å®¹æ€§"""
        self.logger.info("ğŸ” éªŒè¯è§£ç å™¨å…¼å®¹æ€§...")
        
        try:
            expert_config = self.config.get('expert_system', {})
            
            # æµ‹è¯•è§£ç å™¨åˆå§‹åŒ–
            decoder = GenerativeDecoder(
                num_items=1000,  # æµ‹è¯•å€¼
                embedding_dim=64,
                num_layers=4,
                num_heads=4,
                ffn_hidden_dim=256,
                max_seq_len=50,
                expert_config=expert_config
            )
            
            self.logger.info("âœ… è§£ç å™¨åˆå§‹åŒ–æˆåŠŸ")
            
            # æ£€æŸ¥å¯ç”¨çš„ä¸“å®¶
            enabled_experts = decoder.enabled_experts
            self.logger.info(f"ğŸ“‹ å½“å‰å¯ç”¨çš„ä¸“å®¶: {enabled_experts}")
            
            # æµ‹è¯•å‰å‘ä¼ æ’­
            batch_size, seq_len = 2, 10
            target_ids = torch.randint(0, 1000, (batch_size, seq_len))
            encoder_output = torch.randn(batch_size, seq_len, 64)
            memory_padding_mask = torch.zeros(batch_size, seq_len, dtype=torch.bool)
            
            with torch.no_grad():
                logits, weights, balancing_loss = decoder(
                    target_ids, encoder_output, memory_padding_mask,
                    return_weights=True
                )
            
            self.logger.info(f"âœ… å‰å‘ä¼ æ’­æµ‹è¯•æˆåŠŸ - logits shape: {logits.shape}")
            self.logger.info(f"âœ… ä¸“å®¶æƒé‡ shape: {weights.shape if weights is not None else 'None'}")
            self.logger.info(f"âœ… å¹³è¡¡æŸå¤±: {balancing_loss.item()}")
            
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ è§£ç å™¨å…¼å®¹æ€§éªŒè¯å¤±è´¥: {e}")
            return False
    
    def check_expert_system_config(self):
        """æ£€æŸ¥ä¸“å®¶ç³»ç»Ÿé…ç½®"""
        self.logger.info("ğŸ§  æ£€æŸ¥ä¸“å®¶ç³»ç»Ÿé…ç½®...")
        
        expert_config = self.config.get('expert_system', {})
        experts = expert_config.get('experts', {})
        
        self.logger.info("ğŸ“‹ ä¸“å®¶é…ç½®çŠ¶æ€:")
        for expert_name, enabled in experts.items():
            status = "âœ… å¯ç”¨" if enabled else "âŒ ç¦ç”¨"
            self.logger.info(f"  - {expert_name}: {status}")
        
        # æ£€æŸ¥å¿…è¦çš„åµŒå…¥è·¯å¾„
        data_dir = self.config['data']['data_dir']
        
        if experts.get('content_expert', False):
            text_embedding_path = data_dir / "book_gemini_embeddings_filtered_migrated.npy"
            if text_embedding_path.exists():
                self.logger.info("âœ… æ–‡æœ¬åµŒå…¥æ–‡ä»¶å­˜åœ¨")
            else:
                self.logger.warning(f"âš ï¸  æ–‡æœ¬åµŒå…¥æ–‡ä»¶ç¼ºå¤±: {text_embedding_path}")
        
        if experts.get('image_expert', False):
            image_embedding_path = data_dir / "book_image_embeddings_migrated.npy"
            if image_embedding_path.exists():
                self.logger.info("âœ… å›¾åƒåµŒå…¥æ–‡ä»¶å­˜åœ¨")
            else:
                self.logger.warning(f"âš ï¸  å›¾åƒåµŒå…¥æ–‡ä»¶ç¼ºå¤±: {image_embedding_path}")
        
        return expert_config
    
    def generate_compatible_experiment_configs(self):
        """ç”Ÿæˆå…¼å®¹çš„å®éªŒé…ç½®"""
        self.logger.info("âš™ï¸  ç”Ÿæˆå…¼å®¹çš„å®éªŒé…ç½®...")
        
        # åŸºäºå½“å‰è§£ç å™¨çŠ¶æ€çš„å®éªŒé…ç½®
        experiment_configs = {
            "core_ablation": [
                {
                    "name": "baseline_behavior_only",
                    "description": "ä»…è¡Œä¸ºä¸“å®¶ï¼ˆä¼ ç»Ÿæ¨èåŸºçº¿ï¼‰",
                    "args": ["--disable_content_expert", "--disable_image_expert"],
                    "expected_experts": ["behavior_expert"]
                },
                {
                    "name": "behavior_plus_content", 
                    "description": "è¡Œä¸ºä¸“å®¶ + å†…å®¹ä¸“å®¶",
                    "args": ["--disable_image_expert"],
                    "expected_experts": ["behavior_expert", "content_expert"]
                },
                {
                    "name": "behavior_plus_image",
                    "description": "è¡Œä¸ºä¸“å®¶ + å›¾åƒä¸“å®¶",
                    "args": ["--disable_content_expert", "--enable_image_expert"],
                    "expected_experts": ["behavior_expert", "image_expert"]
                },
                {
                    "name": "all_experts",
                    "description": "å…¨ä¸“å®¶ç³»ç»Ÿ",
                    "args": ["--enable_image_expert"],
                    "expected_experts": ["behavior_expert", "content_expert", "image_expert"]
                }
            ],
            
            "architecture_tests": [
                {
                    "name": "frozen_encoder",
                    "description": "å†»ç»“ç¼–ç å™¨æµ‹è¯•",
                    "args": ["--freeze_encoder", "--enable_image_expert"],
                    "expected_experts": ["behavior_expert", "content_expert", "image_expert"]
                },
                {
                    "name": "finetuned_encoder",
                    "description": "ç«¯åˆ°ç«¯å¾®è°ƒæµ‹è¯•",
                    "args": ["--enable_image_expert"],
                    "expected_experts": ["behavior_expert", "content_expert", "image_expert"]
                }
            ]
        }
        
        # ä¿å­˜é…ç½®
        config_file = self.base_dir / "experiments" / "adapted_configs.json"
        with open(config_file, 'w') as f:
            json.dump(experiment_configs, f, indent=2)
        
        self.logger.info(f"âœ… å®éªŒé…ç½®å·²ä¿å­˜è‡³: {config_file}")
        return experiment_configs
    
    def run_compatibility_check(self):
        """è¿è¡Œå®Œæ•´çš„å…¼å®¹æ€§æ£€æŸ¥"""
        self.logger.info("ğŸ”§ å¼€å§‹å…¼å®¹æ€§æ£€æŸ¥...")
        
        checks = [
            ("è§£ç å™¨å…¼å®¹æ€§", self.verify_decoder_compatibility),
            ("ä¸“å®¶ç³»ç»Ÿé…ç½®", lambda: self.check_expert_system_config() is not None),
            ("å®éªŒé…ç½®ç”Ÿæˆ", lambda: self.generate_compatible_experiment_configs() is not None)
        ]
        
        results = {}
        for check_name, check_func in checks:
            try:
                result = check_func()
                results[check_name] = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
                self.logger.info(f"{check_name}: {results[check_name]}")
            except Exception as e:
                results[check_name] = f"âŒ å¼‚å¸¸: {e}"
                self.logger.error(f"{check_name}: {results[check_name]}")
        
        # ç”Ÿæˆå…¼å®¹æ€§æŠ¥å‘Š
        report_path = self.base_dir / "experiments" / "compatibility_report.json"
        report = {
            "timestamp": datetime.now().isoformat(),
            "checks": results,
            "decoder_status": "compatible" if results.get("è§£ç å™¨å…¼å®¹æ€§") == "âœ… é€šè¿‡" else "incompatible",
            "recommendations": self.generate_recommendations(results)
        }
        
        with open(report_path, 'w') as f:
            json.dump(report, f, indent=2)
        
        self.logger.info(f"ğŸ“Š å…¼å®¹æ€§æŠ¥å‘Šå·²ä¿å­˜è‡³: {report_path}")
        return report
    
    def generate_recommendations(self, check_results):
        """åŸºäºæ£€æŸ¥ç»“æœç”Ÿæˆå»ºè®®"""
        recommendations = []
        
        if check_results.get("è§£ç å™¨å…¼å®¹æ€§") != "âœ… é€šè¿‡":
            recommendations.append("éœ€è¦ä¿®å¤è§£ç å™¨å…¼å®¹æ€§é—®é¢˜")
        
        if check_results.get("ä¸“å®¶ç³»ç»Ÿé…ç½®") != "âœ… é€šè¿‡":
            recommendations.append("éœ€è¦æ£€æŸ¥ä¸“å®¶ç³»ç»Ÿé…ç½®å’Œæ•°æ®æ–‡ä»¶")
        
        if not recommendations:
            recommendations.append("ç³»ç»Ÿå…¼å®¹æ€§è‰¯å¥½ï¼Œå¯ä»¥å¼€å§‹å®éªŒ")
        
        return recommendations

def main():
    print("ğŸ”§ GENIUS-Rec å®éªŒé€‚é…å™¨å¯åŠ¨")
    print("="*50)
    
    adapter = ExperimentAdapter()
    report = adapter.run_compatibility_check()
    
    print("\n" + "="*50)
    print("ğŸ“Š å…¼å®¹æ€§æ£€æŸ¥å®Œæˆ")
    
    if report["decoder_status"] == "compatible":
        print("âœ… ç³»ç»Ÿå…¼å®¹æ€§è‰¯å¥½ï¼")
        print("\næ¨èçš„ä¸‹ä¸€æ­¥æ“ä½œ:")
        print("1. è¿è¡Œå¿«é€ŸéªŒè¯: python experiments/quick_validation.py")
        print("2. è¿è¡Œä¸“å®¶æ¶ˆè: python experiments/run_experiments.py --experiment_suite expert_ablation")
        print("3. æŸ¥çœ‹ç»“æœåˆ†æ: python experiments/analyze_results.py")
    else:
        print("âŒ å‘ç°å…¼å®¹æ€§é—®é¢˜ï¼Œè¯·æŸ¥çœ‹æ—¥å¿—è§£å†³åé‡è¯•")
    
    print(f"\nğŸ“‹ è¯¦ç»†æŠ¥å‘Š: {adapter.base_dir}/experiments/compatibility_report.json")
    print(f"ğŸ“‹ é€‚é…æ—¥å¿—: {adapter.adapter_log}")

if __name__ == "__main__":
    main()
