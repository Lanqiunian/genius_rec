# upgrade_checkpoint.py (æœ€ç»ˆä¿®å¤ç‰ˆ)

import torch
import os
from pathlib import Path

# ==============================================================================
#  é…ç½®åŒºåŸŸï¼šè¯·æ ¹æ®éœ€è¦ä¿®æ”¹è¿™é‡Œçš„è·¯å¾„å’Œå°ºå¯¸
# ==============================================================================

# 1. å®šä¹‰æ£€æŸ¥ç‚¹æ‰€åœ¨çš„ç›®å½•
CHECKPOINT_DIR = Path(__file__).parent.parent / "checkpoints"

# 2. å®šä¹‰è¾“å…¥å’Œè¾“å‡ºæ–‡ä»¶å
INPUT_CHECKPOINT_NAME = "hstu_encoder4win_old.pth"
OUTPUT_CHECKPOINT_NAME = "hstu_encoder4win_upgraded.pth"

# 3. å®šä¹‰æ–°æ—§è¯æ±‡è¡¨çš„ç‰¹æ®Šç¬¦å·æ•°é‡
OLD_NUM_SPECIAL_TOKENS = 1  # å‡è®¾æ—§çš„æ£€æŸ¥ç‚¹åªæœ‰1ä¸ªç‰¹æ®Štoken (<PAD>)
NEW_NUM_SPECIAL_TOKENS = 4  # å½“å‰æ¨¡å‹æœ‰4ä¸ªç‰¹æ®Štoken

# 4. å®šä¹‰æ–°çš„æ€»è¯æ±‡è¡¨å¤§å°
NEW_TOTAL_VOCAB_SIZE = 506949

# 5. åµŒå…¥å±‚åœ¨state_dictä¸­çš„é”®å
EMBEDDING_KEY = "item_embedding.weight"

# ==============================================================================

def upgrade_checkpoint_embedding_size():
    """
    ä¸€ä¸ªç‹¬ç«‹çš„è„šæœ¬ï¼Œç”¨äºå‡çº§æ£€æŸ¥ç‚¹ä¸­åµŒå…¥å±‚å°ºå¯¸ï¼Œå¹¶æ ¹æ®æ–°çš„ç‰¹æ®Štokenæ•°é‡ï¼Œ
    æ­£ç¡®åœ°è¿ç§»ç‰©å“å‘é‡çš„ä½ç½®ã€‚
    """
    print("--- å¼€å§‹å‡çº§æ£€æŸ¥ç‚¹ (æœ€ç»ˆä¿®å¤ç‰ˆ) ---")
    
    input_path = CHECKPOINT_DIR / INPUT_CHECKPOINT_NAME
    output_path = CHECKPOINT_DIR / OUTPUT_CHECKPOINT_NAME

    if not input_path.exists():
        print(f"âŒ é”™è¯¯ï¼šè¾“å…¥æ–‡ä»¶æœªæ‰¾åˆ° -> {input_path}")
        return

    print(f"â–¶ï¸ æ­£åœ¨åŠ è½½æ—§çš„æ£€æŸ¥ç‚¹: {input_path}")
    checkpoint = torch.load(input_path, map_location='cpu')

    state_dict = checkpoint.get('model_state_dict', checkpoint)
    
    if EMBEDDING_KEY not in state_dict:
        print(f"âŒ é”™è¯¯ï¼šåœ¨æ£€æŸ¥ç‚¹ä¸­æœªæ‰¾åˆ°é”®å '{EMBEDDING_KEY}'ã€‚")
        return

    old_embedding_tensor = state_dict[EMBEDDING_KEY]
    old_vocab_size, embedding_dim = old_embedding_tensor.shape
    
    print(f"   - æ—§çš„è¯æ±‡è¡¨å¤§å°: {old_vocab_size}")
    print(f"   - æ–°çš„ç›®æ ‡å¤§å°: {NEW_TOTAL_VOCAB_SIZE}")
    print(f"   - åµŒå…¥ç»´åº¦: {embedding_dim}")

    if old_vocab_size >= NEW_TOTAL_VOCAB_SIZE:
        print("âœ… è­¦å‘Šï¼šæ—§çš„æ£€æŸ¥ç‚¹å°ºå¯¸å¤§äºæˆ–ç­‰äºç›®æ ‡å°ºå¯¸ï¼Œæ— éœ€å‡çº§ã€‚")
        return

    # åˆ›å»ºä¸€ä¸ªæ–°çš„ã€å°ºå¯¸æ­£ç¡®çš„åµŒå…¥å±‚å¼ é‡
    new_embedding_tensor = torch.empty(NEW_TOTAL_VOCAB_SIZE, embedding_dim, dtype=old_embedding_tensor.dtype)
    torch.nn.init.normal_(new_embedding_tensor, mean=0, std=0.02)
    print(f"   - å·²åˆ›å»ºæ–°çš„å°ºå¯¸ä¸º [{NEW_TOTAL_VOCAB_SIZE}, {embedding_dim}] çš„åµŒå…¥å±‚ã€‚")

    # --- ã€æ ¸å¿ƒä¿®å¤ã€‘æ‰§è¡Œæ­£ç¡®çš„è¿ç§»é€»è¾‘ ---
    # 1. è®¡ç®—æ—§çš„çœŸå®ç‰©å“åµŒå…¥éƒ¨åˆ†
    old_items_embedding = old_embedding_tensor[OLD_NUM_SPECIAL_TOKENS:]
    num_old_items = old_items_embedding.shape[0]

    # 2. è®¡ç®—æ–°åµŒå…¥çŸ©é˜µä¸­ï¼Œå¯ä»¥è¢«å¡«å……çš„ç‰©å“æ•°é‡
    num_new_items_to_fill = min(num_old_items, NEW_TOTAL_VOCAB_SIZE - NEW_NUM_SPECIAL_TOKENS)

    # 3. æ‰§è¡Œç²¾ç¡®çš„æ‹·è´
    #    å°†æ—§çš„ç‰©å“å‘é‡ï¼Œæ‹·è´åˆ°æ–°çŸ©é˜µä¸­ä»ç¬¬4ä¸ªä½ç½®å¼€å§‹çš„åœ°æ–¹
    new_embedding_tensor[NEW_NUM_SPECIAL_TOKENS : NEW_NUM_SPECIAL_TOKENS + num_new_items_to_fill] = \
        old_items_embedding[:num_new_items_to_fill]
    
    print(f"   - âœ… å·²æˆåŠŸå°† {num_new_items_to_fill} ä¸ªçœŸå®ç‰©å“çš„æƒé‡ï¼Œä»æ—§ç´¢å¼• {OLD_NUM_SPECIAL_TOKENS}: "
          f"æ‹·è´åˆ°æ–°ç´¢å¼• {NEW_NUM_SPECIAL_TOKENS}:")

    # ç”¨æ–°çš„åµŒå…¥å±‚æƒé‡æ›¿æ¢æ‰æ—§çš„
    state_dict[EMBEDDING_KEY] = new_embedding_tensor

    print(f"ğŸ’¾ æ­£åœ¨ä¿å­˜å‡çº§åçš„æ£€æŸ¥ç‚¹åˆ°: {output_path}")
    torch.save(checkpoint, output_path)

    print("--- âœ… æ£€æŸ¥ç‚¹å‡çº§æˆåŠŸï¼ ---")


if __name__ == '__main__':
    CHECKPOINT_DIR.mkdir(exist_ok=True)
    upgrade_checkpoint_embedding_size()