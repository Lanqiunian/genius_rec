# upgrade_checkpoint.py (å·²ä¿®å¤BUGçš„æœ€ç»ˆç‰ˆ)

import torch
import os
from pathlib import Path

# ==============================================================================
#   é…ç½®åŒºåŸŸï¼šè¯·æ ¹æ®éœ€è¦ä¿®æ”¹è¿™é‡Œçš„è·¯å¾„å’Œå‚æ•°
# ==============================================================================

# 1. å®šä¹‰æ£€æŸ¥ç‚¹æ‰€åœ¨çš„ç›®å½•
CHECKPOINT_DIR = Path(__file__).parent.parent / "checkpoints"

# 2. å®šä¹‰è¾“å…¥å’Œè¾“å‡ºæ–‡ä»¶å
INPUT_CHECKPOINT_NAME = "hstu_encoder4win_old.pth"
OUTPUT_CHECKPOINT_NAME = "hstu_encoder_upgraded.pth" # å»ºè®®ä½¿ç”¨æ–°åå­—ä»¥é¿å…æ··æ·†

# 3. å®šä¹‰æ–°æ—§è¯æ±‡è¡¨çš„ç‰¹æ®Šç¬¦å·æ•°é‡
OLD_NUM_SPECIAL_TOKENS = 1  # æ—§æ£€æŸ¥ç‚¹åªæœ‰1ä¸ªç‰¹æ®Štoken (<PAD>)
NEW_NUM_SPECIAL_TOKENS = 4  # å½“å‰æ¨¡å‹æœ‰4ä¸ªç‰¹æ®Štoken

# 4. å®šä¹‰æ–°çš„æ€»è¯æ±‡è¡¨å¤§å°
NEW_TOTAL_VOCAB_SIZE = 506949

# 5. åµŒå…¥å±‚åœ¨state_dictä¸­çš„é”®å
EMBEDDING_KEY = "item_embedding.weight"

# ==============================================================================

def upgrade_checkpoint_embedding_size_fixed():
    """
    ä¸€ä¸ªç‹¬ç«‹çš„ã€å·²ä¿®å¤BUGçš„è„šæœ¬ï¼Œç”¨äºç²¾ç¡®å‡çº§æ£€æŸ¥ç‚¹ä¸­åµŒå…¥å±‚çš„å°ºå¯¸ï¼Œ
    å¹¶æ ¹æ®æ–°çš„ç‰¹æ®Štokenæ•°é‡ï¼Œæ­£ç¡®åœ°ã€åˆ†æ®µåœ°è¿ç§»æƒé‡ã€‚
    """
    print("--- å¼€å§‹å‡çº§æ£€æŸ¥ç‚¹ (å·²ä¿®å¤BUGçš„æœ€ç»ˆç‰ˆ) ---")
    
    input_path = CHECKPOINT_DIR / INPUT_CHECKPOINT_NAME
    output_path = CHECKPOINT_DIR / OUTPUT_CHECKPOINT_NAME

    if not input_path.exists():
        print(f"âŒ é”™è¯¯ï¼šè¾“å…¥æ–‡ä»¶æœªæ‰¾åˆ° -> {input_path}")
        return

    print(f"â–¶ï¸ æ­£åœ¨åŠ è½½æ—§çš„æ£€æŸ¥ç‚¹: {input_path}")
    # åŠ è½½æ—¶æ˜ç¡®æŒ‡å®š weights_only=False ä»¥è¯»å–åŒ…å«pickleå¯¹è±¡çš„æ—§æ–‡ä»¶
    checkpoint = torch.load(input_path, map_location='cpu', weights_only=False)

    # å…¼å®¹ä¸¤ç§æ£€æŸ¥ç‚¹æ ¼å¼ï¼šç›´æ¥çš„state_dictæˆ–åŒ…å«å®ƒçš„å¤–å±‚å­—å…¸
    state_dict = checkpoint.get('model_state_dict', checkpoint)
    
    if EMBEDDING_KEY not in state_dict:
        print(f"âŒ é”™è¯¯ï¼šåœ¨æ£€æŸ¥ç‚¹ä¸­æœªæ‰¾åˆ°é”®å '{EMBEDDING_KEY}'ã€‚")
        return

    old_embedding_tensor = state_dict[EMBEDDING_KEY]
    old_vocab_size, embedding_dim = old_embedding_tensor.shape
    
    print(f"   - æ—§çš„è¯æ±‡è¡¨å¤§å°: {old_vocab_size}")
    print(f"   - æ–°çš„ç›®æ ‡å¤§å°: {NEW_TOTAL_VOCAB_SIZE}")
    print(f"   - åµŒå…¥ç»´åº¦: {embedding_dim}")

    if old_vocab_size >= NEW_TOTAL_VOCAB_SIZE:
        print("âœ… è­¦å‘Šï¼šæ—§çš„æ£€æŸ¥ç‚¹å°ºå¯¸å¤§äºæˆ–ç­‰äºç›®æ ‡å°ºå¯¸ï¼Œæ— éœ€å‡çº§ã€‚")
        return

    # --- ã€æ ¸å¿ƒä¿®å¤ã€‘åˆ›å»ºé›¶åˆå§‹åŒ–çš„æ–°çŸ©é˜µï¼Œå¹¶åˆ†æ®µè¿ç§» ---
    
    # 1. åˆ›å»ºä¸€ä¸ªå…¨é›¶çš„æ–°çŸ©é˜µï¼Œè¿™å¯¹äºPAD tokenæ˜¯æ­£ç¡®çš„
    new_embedding_tensor = torch.zeros(NEW_TOTAL_VOCAB_SIZE, embedding_dim, dtype=old_embedding_tensor.dtype)
    print(f"   - å·²åˆ›å»ºæ–°çš„é›¶åˆå§‹åŒ–å°ºå¯¸ä¸º [{NEW_TOTAL_VOCAB_SIZE}, {embedding_dim}] çš„åµŒå…¥å±‚ã€‚")

    # 2. è¿ç§»ç‰¹æ®ŠTokens (ä¾‹å¦‚, å°†æ—§çš„PAD tokenå‘é‡æ‹·è´åˆ°æ–°PAD tokençš„ä½ç½®)
    num_special_to_copy = min(OLD_NUM_SPECIAL_TOKENS, NEW_NUM_SPECIAL_TOKENS)
    new_embedding_tensor[:num_special_to_copy] = old_embedding_tensor[:num_special_to_copy]
    print(f"   - âœ… å·²ç²¾ç¡®æ‹·è´ {num_special_to_copy} ä¸ªç‰¹æ®ŠTokençš„æƒé‡ã€‚")

    # 3. è¿ç§»æ‰€æœ‰çœŸå®ç‰©å“çš„åµŒå…¥å‘é‡
    old_items_embedding = old_embedding_tensor[OLD_NUM_SPECIAL_TOKENS:]
    num_items_to_copy = min(old_items_embedding.shape[0], NEW_TOTAL_VOCAB_SIZE - NEW_NUM_SPECIAL_TOKENS)

    new_embedding_tensor[NEW_NUM_SPECIAL_TOKENS : NEW_NUM_SPECIAL_TOKENS + num_items_to_copy] = \
        old_items_embedding[:num_items_to_copy]
    
    print(f"   - âœ… å·²ç²¾ç¡®å°† {num_items_to_copy} ä¸ªçœŸå®ç‰©å“çš„æƒé‡ä»æ—§ç´¢å¼•[{OLD_NUM_SPECIAL_TOKENS}:]æ‹·è´åˆ°æ–°ç´¢å¼•[{NEW_NUM_SPECIAL_TOKENS}:]")

    # ç”¨ä¿®å¤åçš„æ–°åµŒå…¥å±‚æƒé‡æ›¿æ¢æ‰æ—§çš„
    state_dict[EMBEDDING_KEY] = new_embedding_tensor

    # æ ¹æ®åŸå§‹æ£€æŸ¥ç‚¹æ ¼å¼ï¼Œæ­£ç¡®åœ°ä¿å­˜
    if 'model_state_dict' in checkpoint:
        checkpoint['model_state_dict'] = state_dict
        torch.save(checkpoint, output_path)
    else:
        torch.save(state_dict, output_path)

    print(f"ğŸ’¾ æ­£åœ¨ä¿å­˜ä¿®å¤åçš„æ£€æŸ¥ç‚¹åˆ°: {output_path}")
    print("--- âœ… æ£€æŸ¥ç‚¹å‡çº§æˆåŠŸï¼ç°åœ¨å¯ä»¥å®‰å…¨ä½¿ç”¨æ–°æ–‡ä»¶è¿›è¡Œè®­ç»ƒã€‚ ---")


if __name__ == '__main__':
    CHECKPOINT_DIR.mkdir(exist_ok=True)
    upgrade_checkpoint_embedding_size_fixed()