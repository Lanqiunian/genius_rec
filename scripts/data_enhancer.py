import pandas as pd
import os
import json
import pickle
import requests
import time
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed
from multiprocessing import Pool, cpu_count

# =============================================================================
# è¾…åŠ©å‡½æ•°ï¼ˆä¸‹è½½ã€è¡Œå¤„ç†ã€åˆ†å—ï¼‰
# =============================================================================

def process_chunk(chunk_data):
    """å·¥ä½œè¿›ç¨‹å‡½æ•°ï¼šå¤„ç†ä¼ å…¥çš„å­—èŠ‚æ•°æ®å—ã€‚"""
    results = []
    for line in chunk_data.decode('utf-8').splitlines():
        if not line:
            continue
        try:
            item = json.loads(line.strip())
            asin = item.get('parent_asin')
            title = item.get('title')
            if not asin or not title:
                continue

            subtitle = item.get('subtitle', '')
            description_list = item.get('description', [])
            description_text = ' '.join(description_list) if isinstance(description_list, list) else ''
            features_list = item.get('features', [])
            features_text = ' '.join(features_list) if isinstance(features_list, list) else ''
            full_text = f"æ ‡é¢˜: {title} <SEP> å‰¯æ ‡é¢˜: {subtitle} <SEP> ç‰¹ç‚¹: {features_text} <SEP> æè¿°: {description_text}"

            image_url = None
            image_list = item.get('images', [])
            if image_list and isinstance(image_list, list):
                for img_info in image_list:
                    if isinstance(img_info, dict) and 'large' in img_info and img_info['large']:
                        image_url = img_info['large']
                        break
            
            results.append({
                'asin': asin,
                'full_text': ' '.join(full_text.split()),
                'image_url': image_url
            })
        except (json.JSONDecodeError, AttributeError):
            continue
    return results

def read_chunks(file_path):
    """æ™ºèƒ½åœ°å°†å¤§æ–‡ä»¶æŒ‰è¡Œåˆ†å‰²æˆé€‚åˆå¹¶è¡Œå¤„ç†çš„å—ã€‚"""
    file_size = os.path.getsize(file_path)
    num_chunks = min(cpu_count() * 4, int(file_size / (1024 * 1024 * 64)))
    num_chunks = max(num_chunks, 1)
    chunk_size = file_size // num_chunks

    with open(file_path, 'rb') as f:
        start = 0
        while start < file_size:
            end = min(start + chunk_size, file_size)
            if end < file_size:
                f.seek(end)
                f.readline()
                end = f.tell()
            f.seek(start)
            chunk_data = f.read(end - start)
            yield chunk_data
            start = end
            if start >= file_size:
                break

def download_image_task(url, save_path, asin, max_retries=2):
    """å•ä¸ªå›¾ç‰‡ä¸‹è½½ä»»åŠ¡ï¼Œå¸¦é‡è¯•æœºåˆ¶"""
    if not url or not isinstance(url, str) or not url.startswith('http'):
        return False, asin
    
    for attempt in range(max_retries):
        try:
            # è®¾ç½®è¯·æ±‚å¤´æ¨¡æ‹Ÿæµè§ˆå™¨
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                'Accept': 'image/webp,image/apng,image/*,*/*;q=0.8',
            }
            
            response = requests.get(url, timeout=15, stream=True, headers=headers)
            response.raise_for_status()
            
            # æ£€æŸ¥å†…å®¹ç±»å‹
            content_type = response.headers.get('content-type', '').lower()
            if not any(img_type in content_type for img_type in ['image/', 'jpeg', 'jpg', 'png']):
                continue  # å°è¯•ä¸‹æ¬¡é‡è¯•
            
            with open(save_path, 'wb') as handler:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        handler.write(chunk)
            
            # éªŒè¯æ–‡ä»¶å¤§å°
            if os.path.getsize(save_path) < 1024:
                os.remove(save_path)
                continue  # å°è¯•ä¸‹æ¬¡é‡è¯•
            
            return True, asin
            
        except requests.RequestException:
            if attempt < max_retries - 1:
                time.sleep(0.5 * (attempt + 1))  # é€’å¢å»¶è¿Ÿ
                continue
    
    return False, asin

# =============================================================================
# ä¸»é€»è¾‘å‡½æ•°
# =============================================================================

def create_full_text_parquet(metadata_path, output_path):
    """ç¬¬ä¸€é˜¶æ®µï¼šå¤„ç†å…¨é‡å…ƒæ•°æ®ï¼Œç”Ÿæˆæ–‡æœ¬Parquetæ–‡ä»¶"""
    print("--- é˜¶æ®µ 1: å¼€å§‹å¤„ç†å…¨é‡å…ƒæ•°æ® ---")
    if not os.path.exists(metadata_path):
        print(f"é”™è¯¯ï¼šæ‰¾ä¸åˆ°å…ƒæ•°æ®æ–‡ä»¶ '{metadata_path}'ã€‚")
        print("è¯·å…ˆæ‰§è¡Œ 'gzip -d meta_Books.jsonl.gz' å‘½ä»¤è§£å‹æ–‡ä»¶ã€‚")
        return False
    
    tasks = list(read_chunks(metadata_path))
    all_results = []

    with Pool(processes=cpu_count()) as pool:
        with tqdm(total=len(tasks), desc="å¤šè¿›ç¨‹è§£æè¿›åº¦") as pbar:
            for result in pool.imap_unordered(process_chunk, tasks):
                all_results.extend(result)
                pbar.update(1)

    all_books_df = pd.DataFrame(all_results)
    all_books_df.to_parquet(output_path, engine='pyarrow', index=False)
    print(f"\nâœ… é˜¶æ®µ 1 å®Œæˆï¼å…± {len(all_books_df)} æ¡è®°å½•å·²ä¿å­˜è‡³ '{output_path}'")
    return True

def download_filtered_images(text_parquet_path, image_dir, id_maps_path, max_workers):
    """ç¬¬äºŒé˜¶æ®µï¼šåŸºäº5-coreè¿‡æ»¤åçš„ç‰©å“å­é›†ä¸‹è½½å›¾ç‰‡"""
    print(f"\n--- é˜¶æ®µ 2: å¼€å§‹ä¸‹è½½5-coreè¿‡æ»¤åå­é›†çš„å›¾ç‰‡ ---")
    
    # æ£€æŸ¥å¿…è¦æ–‡ä»¶
    if not os.path.exists(text_parquet_path):
        print(f"é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ–‡æœ¬æ•°æ®æ–‡ä»¶ '{text_parquet_path}'ã€‚è¯·å…ˆå®Œæˆé˜¶æ®µ1ã€‚")
        return
    
    if not os.path.exists(id_maps_path):
        print(f"é”™è¯¯ï¼šæ‰¾ä¸åˆ°IDæ˜ å°„æ–‡ä»¶ '{id_maps_path}'ã€‚")
        return

    if not os.path.exists(image_dir):
        os.makedirs(image_dir)
        print(f"å·²åˆ›å»ºå›¾ç‰‡ä¿å­˜ç›®å½•: {image_dir}")

    # åŠ è½½5-coreè¿‡æ»¤åçš„ç‰©å“åˆ—è¡¨
    print("æ­£åœ¨åŠ è½½5-coreè¿‡æ»¤åçš„ç‰©å“æ˜ å°„...")
    with open(id_maps_path, 'rb') as f:
        id_maps = pickle.load(f)
    
    valid_asins = set(id_maps['item_map'].keys())
    print(f"5-coreè¿‡æ»¤åæœ‰æ•ˆç‰©å“æ•°é‡: {len(valid_asins)}")

    # åŠ è½½å…¨é‡æ–‡æœ¬æ•°æ®å¹¶è¿‡æ»¤
    print("æ­£åœ¨åŠ è½½å’Œè¿‡æ»¤æ–‡æœ¬æ•°æ®...")
    df = pd.read_parquet(text_parquet_path)
    
    # åªä¿ç•™åœ¨5-coreå­é›†ä¸­çš„ç‰©å“
    filtered_df = df[df['asin'].isin(valid_asins)].dropna(subset=['image_url'])
    print(f"è¿‡æ»¤åæœ‰å›¾ç‰‡URLçš„ç‰©å“æ•°é‡: {len(filtered_df)}")

    # ç”Ÿæˆä¸‹è½½ä»»åŠ¡åˆ—è¡¨ï¼Œæ–‡ä»¶åä½¿ç”¨ASIN
    download_tasks = []
    asin_to_itemid = {}  # è®°å½•ASINåˆ°item_idçš„æ˜ å°„
    
    for _, row in filtered_df.iterrows():
        asin = row['asin']
        item_id = id_maps['item_map'][asin]
        image_path = os.path.join(image_dir, f"{asin}.jpg")
        
        # å¦‚æœå›¾ç‰‡è¿˜ä¸å­˜åœ¨ï¼ŒåŠ å…¥ä¸‹è½½ä»»åŠ¡
        if not os.path.exists(image_path):
            download_tasks.append((row['image_url'], image_path, asin))
        
        # è®°å½•æ˜ å°„å…³ç³»
        asin_to_itemid[asin] = item_id
    
    # ä¿å­˜ASINåˆ°item_idçš„æ˜ å°„æ–‡ä»¶ï¼Œä¾›åç»­embeddingç”Ÿæˆä½¿ç”¨
    mapping_file = os.path.join(image_dir, 'asin_to_itemid_mapping.pkl')
    with open(mapping_file, 'wb') as f:
        pickle.dump(asin_to_itemid, f)
    print(f"å·²ä¿å­˜ASINåˆ°item_idæ˜ å°„æ–‡ä»¶: {mapping_file}")
    
    if not download_tasks:
        print("æ‰€æœ‰5-coreå­é›†å›¾ç‰‡å‡å·²å­˜åœ¨ï¼Œæ— éœ€ä¸‹è½½ã€‚")
        print("âœ… é˜¶æ®µ 2 å®Œæˆï¼")
        return

    print(f"å…±éœ€ä¸‹è½½ {len(download_tasks)} / {len(filtered_df)} å¼ æ–°å›¾ç‰‡ã€‚")
    successful_downloads = 0
    failed_downloads_asin = []

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {executor.submit(download_image_task, url, path, asin): asin for url, path, asin in download_tasks}
        progress = tqdm(as_completed(futures), total=len(futures), desc="å›¾ç‰‡ä¸‹è½½è¿›åº¦")
        for future in progress:
            success, asin = future.result()
            if success:
                successful_downloads += 1
            else:
                failed_downloads_asin.append(asin)

    print("\n--------------------------------------------------")
    print("âœ… é˜¶æ®µ 2 å›¾ç‰‡ä¸‹è½½å®Œæˆï¼")
    print(f"  - 5-coreæœ‰æ•ˆç‰©å“: {len(valid_asins)}")
    print(f"  - æœ‰å›¾ç‰‡URLçš„ç‰©å“: {len(filtered_df)}")
    print(f"  - æˆåŠŸä¸‹è½½: {successful_downloads} å¼ ")
    print(f"  - ä¸‹è½½å¤±è´¥: {len(failed_downloads_asin)} å¼ ")
    print(f"  - å›¾ç‰‡ä¿å­˜ä½ç½®: {image_dir}")
    print(f"  - æ˜ å°„æ–‡ä»¶: {mapping_file}")
    if failed_downloads_asin:
        print("  - å¤±è´¥çš„ASINåˆ—è¡¨ (å‰20ä¸ª):", failed_downloads_asin[:20])
    print("--------------------------------------------------")

def main():
    # --- å…¨å±€å‚æ•°è®¾ç½® ---
    RAW_METADATA_PATH = 'data/meta_Books.jsonl'
    TEXT_PARQUET_PATH = 'data/book_text_data.parquet'
    IMAGE_OUTPUT_DIR = 'data/book_covers_enhanced'
    ID_MAPS_PATH = 'data/processed/id_maps.pkl'  # 5-coreè¿‡æ»¤åçš„ç‰©å“æ˜ å°„
    
    MAX_DOWNLOAD_WORKERS = 50

    print("=== åŸºäº5-coreè¿‡æ»¤å­é›†çš„å›¾ç‰‡ä¸‹è½½å™¨ ===")
    print(f"è¾“å…¥å…ƒæ•°æ®: {RAW_METADATA_PATH}")
    print(f"æ–‡æœ¬æ•°æ®: {TEXT_PARQUET_PATH}")
    print(f"IDæ˜ å°„æ–‡ä»¶: {ID_MAPS_PATH}")
    print(f"å›¾ç‰‡ä¿å­˜ç›®å½•: {IMAGE_OUTPUT_DIR}")
    print("=" * 50)

    # --- æ‰§è¡Œå·¥ä½œæµ ---
    # å¦‚æœå…¨é‡æ–‡æœ¬æ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ™åˆ›å»ºå®ƒ
    if not os.path.exists(TEXT_PARQUET_PATH):
        success = create_full_text_parquet(RAW_METADATA_PATH, TEXT_PARQUET_PATH)
        if not success:
            return # å¦‚æœç¬¬ä¸€æ­¥å¤±è´¥ï¼Œåˆ™ç»ˆæ­¢
    else:
        print(f"--- é˜¶æ®µ 1: è·³è¿‡ ---")
        print(f"å·²æ‰¾åˆ°å…¨é‡æ–‡æœ¬æ–‡ä»¶ '{TEXT_PARQUET_PATH}'ã€‚")

    # æ‰§è¡Œç¬¬äºŒæ­¥ï¼Œä¸‹è½½5-coreè¿‡æ»¤åçš„å›¾ç‰‡
    download_filtered_images(TEXT_PARQUET_PATH, IMAGE_OUTPUT_DIR, ID_MAPS_PATH, MAX_DOWNLOAD_WORKERS)
    
    print("\nğŸ‰ æ‰€æœ‰ä»»åŠ¡å·²æ‰§è¡Œå®Œæ¯•ï¼")
    print("\nğŸ“‹ ç”Ÿæˆçš„æ–‡ä»¶:")
    print(f"  - å›¾ç‰‡ç›®å½•: {IMAGE_OUTPUT_DIR}/*.jpg")
    print(f"  - æ˜ å°„æ–‡ä»¶: {IMAGE_OUTPUT_DIR}/asin_to_itemid_mapping.pkl")
    print("\nğŸ’¡ ä¸‹ä¸€æ­¥:")
    print("  å¯ä»¥ä½¿ç”¨generate_image_embeddings.pyç”Ÿæˆå›¾åƒåµŒå…¥")
    print(f"  python generate_image_embeddings.py --input_dir {IMAGE_OUTPUT_DIR} --output_file data/book_image_embeddings.npy")

if __name__ == '__main__':
    main()