# generate_image_embeddings.py
"""
ç”Ÿæˆä¹¦å°é¢å›¾åƒåµŒå…¥çš„è„šæœ¬ï¼ˆé¢„ç•™ï¼‰
ç”¨äºä¸ºå›¾åƒä¸“å®¶å‡†å¤‡æ•°æ®

ä½¿ç”¨æ–¹æ³•:
python generate_image_embeddings.py --model_type clip --input_dir data/book_covers_enhanced --output_file data/book_image_embeddings.npy

æ”¯æŒçš„æ¨¡å‹:
- clip: ä½¿ç”¨OpenAI CLIPæ¨¡å‹
- resnet: ä½¿ç”¨ResNet-50é¢„è®­ç»ƒæ¨¡å‹
- vit: ä½¿ç”¨Vision Transformer
"""

import argparse
import os
import pickle
from pathlib import Path
from PIL import Image
import numpy as np
import torch
import torch.nn as nn
from torchvision import transforms
from tqdm import tqdm

try:
    import clip
    CLIP_AVAILABLE = True
except ImportError:
    CLIP_AVAILABLE = False
    print("âš ï¸  CLIP not available. Install with: pip install git+https://github.com/openai/CLIP.git")

try:
    from torchvision.models import resnet50, vit_b_16
    TORCHVISION_AVAILABLE = True
except ImportError:
    TORCHVISION_AVAILABLE = False
    print("âš ï¸  TorchVision not available. Install with: pip install torchvision")


class ImageEmbeddingGenerator:
    """å›¾åƒåµŒå…¥ç”Ÿæˆå™¨"""
    
    def __init__(self, model_type="clip", device="cuda"):
        self.model_type = model_type
        self.device = device
        self.model = None
        self.preprocess = None
        
        self._load_model()
    
    def _load_model(self):
        """åŠ è½½æŒ‡å®šçš„å›¾åƒç¼–ç æ¨¡å‹"""
        if self.model_type == "clip":
            if not CLIP_AVAILABLE:
                raise ImportError("CLIP not available")
            self.model, self.preprocess = clip.load("ViT-B/32", device=self.device)
            self.embedding_dim = 512
            
        elif self.model_type == "resnet":
            if not TORCHVISION_AVAILABLE:
                raise ImportError("TorchVision not available")
            self.model = resnet50(pretrained=True)
            self.model.fc = nn.Identity()  # ç§»é™¤æœ€åçš„åˆ†ç±»å±‚
            self.model = self.model.to(self.device)
            self.model.eval()
            self.embedding_dim = 2048
            
            # ResNeté¢„å¤„ç†
            self.preprocess = transforms.Compose([
                transforms.Resize(256),
                transforms.CenterCrop(224),
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
            ])
            
        elif self.model_type == "vit":
            if not TORCHVISION_AVAILABLE:
                raise ImportError("TorchVision not available")
            self.model = vit_b_16(pretrained=True)
            self.model.heads = nn.Identity()  # ç§»é™¤åˆ†ç±»å¤´
            self.model = self.model.to(self.device)
            self.model.eval()
            self.embedding_dim = 768
            
            # ViTé¢„å¤„ç†
            self.preprocess = transforms.Compose([
                transforms.Resize(224),
                transforms.CenterCrop(224),
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
            ])
        else:
            raise ValueError(f"Unsupported model type: {self.model_type}")
        
        print(f"âœ… åŠ è½½{self.model_type.upper()}æ¨¡å‹æˆåŠŸï¼ŒåµŒå…¥ç»´åº¦: {self.embedding_dim}")
    
    def extract_embedding(self, image_path):
        """ä»å•å¼ å›¾åƒæå–åµŒå…¥"""
        try:
            image = Image.open(image_path).convert("RGB")
            
            if self.model_type == "clip":
                image_tensor = self.preprocess(image).unsqueeze(0).to(self.device)
                with torch.no_grad():
                    embedding = self.model.encode_image(image_tensor)
                    embedding = embedding.cpu().numpy().flatten()
            else:
                image_tensor = self.preprocess(image).unsqueeze(0).to(self.device)
                with torch.no_grad():
                    embedding = self.model(image_tensor)
                    embedding = embedding.cpu().numpy().flatten()
            
            return embedding
            
        except Exception as e:
            print(f"âŒ å¤„ç†å›¾åƒå¤±è´¥ {image_path}: {e}")
            return None
    
    def batch_extract_embeddings(self, image_dir, supported_formats=('.jpg', '.jpeg', '.png', '.bmp')):
        """æ‰¹é‡æå–å›¾åƒåµŒå…¥"""
        image_dir = Path(image_dir)
        if not image_dir.exists():
            raise FileNotFoundError(f"å›¾åƒç›®å½•ä¸å­˜åœ¨: {image_dir}")
        
        # æŸ¥æ‰¾æ‰€æœ‰å›¾åƒæ–‡ä»¶
        image_files = []
        for ext in supported_formats:
            image_files.extend(image_dir.glob(f"*{ext}"))
            image_files.extend(image_dir.glob(f"*{ext.upper()}"))
        
        print(f"ğŸ” æ‰¾åˆ° {len(image_files)} å¼ å›¾åƒ")
        
        embeddings_dict = {}
        failed_count = 0
        
        for image_path in tqdm(image_files, desc="æå–å›¾åƒåµŒå…¥"):
            # ä½¿ç”¨æ–‡ä»¶åä½œä¸ºASINï¼ˆå‡è®¾æ–‡ä»¶åå°±æ˜¯ASINï¼‰
            asin = image_path.stem
            
            embedding = self.extract_embedding(image_path)
            if embedding is not None:
                embeddings_dict[asin] = embedding
            else:
                failed_count += 1
        
        print(f"âœ… æˆåŠŸæå– {len(embeddings_dict)} ä¸ªå›¾åƒåµŒå…¥")
        if failed_count > 0:
            print(f"âš ï¸  å¤±è´¥ {failed_count} ä¸ªå›¾åƒ")
        
        return embeddings_dict


def main():
    parser = argparse.ArgumentParser(description="Generate image embeddings for book covers")
    parser.add_argument('--model_type', type=str, default='clip', 
                       choices=['clip', 'resnet', 'vit'],
                       help='Type of image encoder model')
    parser.add_argument('--input_dir', type=str, required=True,
                       help='Directory containing book cover images')
    parser.add_argument('--output_file', type=str, required=True,
                       help='Output file path for embeddings (.npy)')
    parser.add_argument('--device', type=str, default='cuda',
                       help='Device to use (cuda/cpu)')
    parser.add_argument('--id_maps_file', type=str, 
                       default='data/processed/id_maps.pkl',
                       help='ID maps file for filtering valid ASINs')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥è¾“å…¥ç›®å½•
    if not os.path.exists(args.input_dir):
        print(f"âŒ è¾“å…¥ç›®å½•ä¸å­˜åœ¨: {args.input_dir}")
        return
    
    # åˆå§‹åŒ–å›¾åƒåµŒå…¥ç”Ÿæˆå™¨
    try:
        generator = ImageEmbeddingGenerator(args.model_type, args.device)
    except Exception as e:
        print(f"âŒ æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
        return
    
    # æ‰¹é‡æå–åµŒå…¥
    print("å¼€å§‹æå–å›¾åƒåµŒå…¥...")
    embeddings_dict = generator.batch_extract_embeddings(args.input_dir)
    
    if not embeddings_dict:
        print("âŒ æ²¡æœ‰æˆåŠŸæå–ä»»ä½•åµŒå…¥")
        return
    
    # å¦‚æœæä¾›äº†ID mapsæ–‡ä»¶ï¼Œè¿›è¡Œè¿‡æ»¤
    if os.path.exists(args.id_maps_file):
        print(f"ğŸ” ä½¿ç”¨ID mapsè¿‡æ»¤æœ‰æ•ˆçš„ASIN...")
        with open(args.id_maps_file, 'rb') as f:
            id_maps = pickle.load(f)
        
        valid_asins = set(id_maps['item_map'].keys())
        filtered_embeddings = {asin: emb for asin, emb in embeddings_dict.items() if asin in valid_asins}
        
        print(f"ğŸ“Š è¿‡æ»¤å‰: {len(embeddings_dict)}, è¿‡æ»¤å: {len(filtered_embeddings)}")
        embeddings_dict = filtered_embeddings
    
    # æ£€æŸ¥æ˜¯å¦æœ‰ASINåˆ°item_idçš„æ˜ å°„æ–‡ä»¶
    mapping_file = Path(args.input_dir) / 'asin_to_itemid_mapping.pkl'
    if mapping_file.exists():
        print(f"ğŸ”— å‘ç°ASINåˆ°item_idæ˜ å°„æ–‡ä»¶ï¼Œè½¬æ¢ä¸ºitem_idç´¢å¼•...")
        with open(mapping_file, 'rb') as f:
            asin_to_itemid = pickle.load(f)
        
        # å°†embeddingså­—å…¸çš„é”®ä»ASINè½¬æ¢ä¸ºitem_id
        itemid_embeddings = {}
        converted_count = 0
        for asin, embedding in embeddings_dict.items():
            if asin in asin_to_itemid:
                item_id = asin_to_itemid[asin]
                itemid_embeddings[item_id] = embedding
                converted_count += 1
        
        print(f"ğŸ“ˆ æˆåŠŸè½¬æ¢ {converted_count} ä¸ªåµŒå…¥çš„ç´¢å¼• (ASIN â†’ item_id)")
        embeddings_dict = itemid_embeddings
    
    # ä¿å­˜ç»“æœ
    output_path = Path(args.output_file)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    np.save(output_path, embeddings_dict)
    print(f"âœ… å›¾åƒåµŒå…¥å·²ä¿å­˜åˆ°: {output_path}")
    print(f"ğŸ“Š æœ€ç»ˆåŒ…å« {len(embeddings_dict)} ä¸ªæœ‰æ•ˆçš„å›¾åƒåµŒå…¥")
    print(f"ğŸ¯ åµŒå…¥ç»´åº¦: {generator.embedding_dim}")


if __name__ == "__main__":
    main()
