#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
å›¾åƒåµŒå…¥ç”Ÿæˆå™¨ - ä½¿ç”¨CLIPæ¨¡å‹ä¸ºä¹¦ç±å°é¢ç”ŸæˆåµŒå…¥å‘é‡

åŠŸèƒ½ï¼š
1. æ‰«æbook_covers_enhancedç›®å½•ä¸‹çš„å›¾åƒæ–‡ä»¶
2. ä½¿ç”¨é¢„è®­ç»ƒCLIPæ¨¡å‹ç”Ÿæˆå›¾åƒåµŒå…¥
3. æ”¯æŒæ‰¹å¤„ç†å’ŒGPUåŠ é€Ÿ
4. ç”Ÿæˆbook_image_embeddings.npyåµŒå…¥æ–‡ä»¶

ä½¿ç”¨æ–¹æ³•ï¼š
1. ç¡®ä¿å·²å®‰è£…ä¾èµ–ï¼špip install torch torchvision clip-by-openai pillow tqdm
2. å‡†å¤‡å›¾åƒæ–‡ä»¶åˆ°data/book_covers_enhanced/ç›®å½•
3. è¿è¡Œï¼špython scripts/image_embeddings_generator.py

æ³¨æ„ï¼šéœ€è¦GPUæ”¯æŒä»¥è·å¾—æœ€ä½³æ€§èƒ½
"""

import os
import argparse
import pickle
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from PIL import Image
import clip
from tqdm import tqdm
import gc

class ImageDataset(Dataset):
    """é«˜æ•ˆçš„å›¾åƒæ•°æ®é›†ï¼Œæ”¯æŒå»¶è¿ŸåŠ è½½"""
    
    def __init__(self, image_paths, asins, preprocess):
        self.image_paths = image_paths
        self.asins = asins
        self.preprocess = preprocess
        
    def __len__(self):
        return len(self.image_paths)
    
    def __getitem__(self, idx):
        image_path = self.image_paths[idx]
        asin = self.asins[idx]
        
        try:
            # åŠ è½½å¹¶é¢„å¤„ç†å›¾åƒ
            image = Image.open(image_path).convert('RGB')
            image_tensor = self.preprocess(image)
            return image_tensor, asin, True  # Trueè¡¨ç¤ºåŠ è½½æˆåŠŸ
        except Exception as e:
            # å¦‚æœå›¾åƒæŸåï¼Œè¿”å›é›¶å¼ é‡
            dummy_image = torch.zeros(3, 224, 224)
            return dummy_image, asin, False  # Falseè¡¨ç¤ºåŠ è½½å¤±è´¥

class ImageEmbeddingGenerator:
    """å›¾åƒåµŒå…¥ç”Ÿæˆå™¨"""
    
    def __init__(self, model_type='clip', device='cuda'):
        self.model_type = model_type
        self.device = device
        self.model = None
        self.preprocess = None
        
    def load_model(self):
        """åŠ è½½æŒ‡å®šçš„è§†è§‰æ¨¡å‹"""
        print(f"æ­£åœ¨åŠ è½½ {self.model_type} æ¨¡å‹...")
        
        if self.model_type == 'clip':
            # åŠ è½½CLIPæ¨¡å‹
            model, preprocess = clip.load("ViT-B/32", device=self.device)
            self.model = model
            self.preprocess = preprocess
            print(f"âœ… CLIP ViT-B/32 æ¨¡å‹åŠ è½½æˆåŠŸ (è®¾å¤‡: {self.device})")
            
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {self.model_type}")
    
    def generate_embeddings(self, image_dir, batch_size=32, num_workers=4):
        """
        æ‰¹é‡ç”Ÿæˆå›¾åƒåµŒå…¥
        
        Args:
            image_dir: å›¾åƒç›®å½•è·¯å¾„
            batch_size: æ‰¹å¤„ç†å¤§å° (4090å¯ä»¥ä½¿ç”¨è¾ƒå¤§çš„batch_size)
            num_workers: æ•°æ®åŠ è½½å™¨å·¥ä½œçº¿ç¨‹æ•°
            
        Returns:
            dict: {asin: embedding_vector}
        """
        if self.model is None:
            self.load_model()
            
        # æ‰«æå›¾åƒæ–‡ä»¶
        print("æ­£åœ¨æ‰«æå›¾åƒæ–‡ä»¶...")
        image_paths = []
        asins = []
        
        for filename in os.listdir(image_dir):
            if filename.lower().endswith(('.jpg', '.jpeg', '.png')):
                asin = filename.split('.')[0]  # å»æ‰æ‰©å±•åå¾—åˆ°ASIN
                image_path = os.path.join(image_dir, filename)
                image_paths.append(image_path)
                asins.append(asin)
        
        if not image_paths:
            print("âŒ æœªæ‰¾åˆ°ä»»ä½•å›¾åƒæ–‡ä»¶")
            return {}
            
        print(f"ğŸ“ æ‰¾åˆ° {len(image_paths)} ä¸ªå›¾åƒæ–‡ä»¶")
        
        # åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
        dataset = ImageDataset(image_paths, asins, self.preprocess)
        dataloader = DataLoader(
            dataset, 
            batch_size=batch_size, 
            shuffle=False, 
            num_workers=num_workers,
            pin_memory=True if self.device == 'cuda' else False
        )
        
        # ç”ŸæˆåµŒå…¥
        embeddings_dict = {}
        failed_count = 0
        
        print("å¼€å§‹ç”Ÿæˆå›¾åƒåµŒå…¥...")
        self.model.eval()
        
        with torch.no_grad():
            for batch_images, batch_asins, batch_success in tqdm(dataloader, desc="å¤„ç†å›¾åƒæ‰¹æ¬¡"):
                # ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
                batch_images = batch_images.to(self.device)
                
                # ä½¿ç”¨CLIPæ¨¡å‹ç”Ÿæˆå›¾åƒç‰¹å¾
                if self.model_type == 'clip':
                    image_features = self.model.encode_image(batch_images)
                    # L2å½’ä¸€åŒ–
                    image_features = F.normalize(image_features, p=2, dim=1)
                
                # è½¬æ¢ä¸ºnumpyå¹¶å­˜å‚¨
                image_features_np = image_features.cpu().numpy()
                
                for i, (asin, success) in enumerate(zip(batch_asins, batch_success)):
                    if success:
                        embeddings_dict[asin] = image_features_np[i]
                    else:
                        failed_count += 1
                
                # æ¸…ç†GPUç¼“å­˜
                if self.device == 'cuda':
                    torch.cuda.empty_cache()
        
        print(f"âœ… åµŒå…¥ç”Ÿæˆå®Œæˆ!")
        print(f"   - æˆåŠŸå¤„ç†: {len(embeddings_dict)} ä¸ªå›¾åƒ")
        print(f"   - å¤„ç†å¤±è´¥: {failed_count} ä¸ªå›¾åƒ")
        
        return embeddings_dict


def main():
    """ä¸»å‡½æ•° - è§£æå‚æ•°å¹¶æ‰§è¡Œå›¾åƒåµŒå…¥ç”Ÿæˆ"""
    # é»˜è®¤é…ç½®
    IMAGE_DIR = 'data/book_covers_enhanced'
    OUTPUT_FILE = 'data/book_image_embeddings.npy'
    BATCH_SIZE = 32
    NUM_WORKERS = 4
    
    print("=== å›¾åƒåµŒå…¥ç”Ÿæˆå™¨ ===\n")
    
    # æ£€æŸ¥è¾“å…¥ç›®å½•
    if not os.path.exists(IMAGE_DIR):
        print(f"âŒ å›¾åƒç›®å½•ä¸å­˜åœ¨: {IMAGE_DIR}")
        print("è¯·ç¡®ä¿å·²å°†ä¹¦ç±å°é¢å›¾åƒæ”¾å…¥è¯¥ç›®å½•")
        return
    
    # æ£€æŸ¥CUDAå¯ç”¨æ€§
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    if device == 'cpu':
        print("âš ï¸  æœªæ£€æµ‹åˆ°CUDAï¼Œå°†ä½¿ç”¨CPU (å¤„ç†é€Ÿåº¦è¾ƒæ…¢)")
    
    print(f"é…ç½®ä¿¡æ¯:")
    print(f"  - å›¾åƒç›®å½•: {IMAGE_DIR}")
    print(f"  - è¾“å‡ºæ–‡ä»¶: {OUTPUT_FILE}")
    print(f"  - æ‰¹å¤„ç†å¤§å°: {BATCH_SIZE}")
    print(f"  - è®¡ç®—è®¾å¤‡: {device}")
    
    if device == 'cuda':
        gpu_name = torch.cuda.get_device_name(0)
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        print(f"  - GPU: {gpu_name} ({gpu_memory:.1f}GB)")
    
    print()
    
    try:
        # åˆ›å»ºç”Ÿæˆå™¨å¹¶ç”ŸæˆåµŒå…¥
        generator = ImageEmbeddingGenerator(model_type='clip', device=device)
        embeddings = generator.generate_embeddings(
            image_dir=IMAGE_DIR,
            batch_size=BATCH_SIZE,
            num_workers=NUM_WORKERS
        )
        
        if not embeddings:
            print("âŒ æœªç”Ÿæˆä»»ä½•åµŒå…¥")
            return
        
        # ä¿å­˜åµŒå…¥åˆ°æ–‡ä»¶
        print(f"\næ­£åœ¨ä¿å­˜åµŒå…¥åˆ° {OUTPUT_FILE}...")
        os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)
        np.save(OUTPUT_FILE, embeddings)
        
        print("âœ… å›¾åƒåµŒå…¥ç”Ÿæˆå®Œæˆ!")
        print(f"   - æ€»åµŒå…¥æ•°: {len(embeddings)}")
        sample_embedding = next(iter(embeddings.values()))
        print(f"   - åµŒå…¥ç»´åº¦: {sample_embedding.shape}")
        print(f"   - å·²ä¿å­˜è‡³: {OUTPUT_FILE}")
        
    except Exception as e:
        print(f"âŒ ç”Ÿæˆè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
        print(f"GPUå‹å·: {torch.cuda.get_device_name()}")
        print(f"GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
    print("=" * 50)
    
    # åˆ›å»ºç”Ÿæˆå™¨å¹¶ç”ŸæˆåµŒå…¥
    generator = ImageEmbeddingGenerator(args.model_type, args.device)
    embeddings = generator.generate_embeddings(
        args.input_dir, 
        args.batch_size, 
        args.num_workers
    )
    
    if not embeddings:
        print("âŒ æœªç”Ÿæˆä»»ä½•åµŒå…¥ï¼Œç¨‹åºé€€å‡º")
        return
    
    # å¦‚æœéœ€è¦ï¼Œè½¬æ¢ä¸ºitem_idé”®
    if args.use_item_id_keys:
        mapping_file = os.path.join(args.input_dir, 'asin_to_itemid_mapping.pkl')
        embeddings = map_asins_to_item_ids(embeddings, mapping_file)
    
    # ä¿å­˜åµŒå…¥æ–‡ä»¶
    print(f"\nğŸ’¾ æ­£åœ¨ä¿å­˜åµŒå…¥åˆ°: {args.output_file}")
    os.makedirs(os.path.dirname(args.output_file), exist_ok=True)
    np.save(args.output_file, embeddings, allow_pickle=True)
    
    print("ğŸ‰ å›¾åƒåµŒå…¥ç”Ÿæˆå®Œæˆ!")
    print(f"ğŸ“Š æœ€ç»ˆç»Ÿè®¡:")
    print(f"  - åµŒå…¥æ•°é‡: {len(embeddings)}")
    print(f"  - æ–‡ä»¶å¤§å°: {os.path.getsize(args.output_file) / 1024**2:.1f} MB")
    
    # å†…å­˜æ¸…ç†
    torch.cuda.empty_cache()
    gc.collect()

if __name__ == '__main__':
    main()
