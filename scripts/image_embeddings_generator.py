#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
å›¾åƒåµŒå…¥ç”Ÿæˆå™¨ - æ”¯æŒCLIPç­‰å¤šç§è§†è§‰æ¨¡å‹
ä½¿ç”¨å•å¡4090é«˜æ•ˆç”Ÿæˆå›¾åƒåµŒå…¥ï¼Œæ”¯æŒæ‰¹å¤„ç†å’Œå†…å­˜ä¼˜åŒ–
"""

import os
import argparse
import pickle
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from PIL import Image
import clip
from tqdm import tqdm
import gc

class ImageDataset(Dataset):
    """é«˜æ•ˆçš„å›¾åƒæ•°æ®é›†ï¼Œæ”¯æŒå»¶è¿ŸåŠ è½½"""
    
    def __init__(self, image_paths, asins, preprocess):
        self.image_paths = image_paths
        self.asins = asins
        self.preprocess = preprocess
        
    def __len__(self):
        return len(self.image_paths)
    
    def __getitem__(self, idx):
        image_path = self.image_paths[idx]
        asin = self.asins[idx]
        
        try:
            # åŠ è½½å¹¶é¢„å¤„ç†å›¾åƒ
            image = Image.open(image_path).convert('RGB')
            image_tensor = self.preprocess(image)
            return image_tensor, asin, True  # Trueè¡¨ç¤ºåŠ è½½æˆåŠŸ
        except Exception as e:
            # å¦‚æœå›¾åƒæŸåï¼Œè¿”å›é›¶å¼ é‡
            dummy_image = torch.zeros(3, 224, 224)
            return dummy_image, asin, False  # Falseè¡¨ç¤ºåŠ è½½å¤±è´¥

class ImageEmbeddingGenerator:
    """å›¾åƒåµŒå…¥ç”Ÿæˆå™¨"""
    
    def __init__(self, model_type='clip', device='cuda'):
        self.model_type = model_type
        self.device = device
        self.model = None
        self.preprocess = None
        
    def load_model(self):
        """åŠ è½½æŒ‡å®šçš„è§†è§‰æ¨¡å‹"""
        print(f"æ­£åœ¨åŠ è½½ {self.model_type} æ¨¡å‹...")
        
        if self.model_type == 'clip':
            # åŠ è½½CLIPæ¨¡å‹
            model, preprocess = clip.load("ViT-B/32", device=self.device)
            self.model = model
            self.preprocess = preprocess
            print(f"âœ… CLIP ViT-B/32 æ¨¡å‹åŠ è½½æˆåŠŸ (è®¾å¤‡: {self.device})")
            
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {self.model_type}")
    
    def generate_embeddings(self, image_dir, batch_size=32, num_workers=4):
        """
        æ‰¹é‡ç”Ÿæˆå›¾åƒåµŒå…¥
        
        Args:
            image_dir: å›¾åƒç›®å½•è·¯å¾„
            batch_size: æ‰¹å¤„ç†å¤§å° (4090å¯ä»¥ä½¿ç”¨è¾ƒå¤§çš„batch_size)
            num_workers: æ•°æ®åŠ è½½å™¨å·¥ä½œçº¿ç¨‹æ•°
            
        Returns:
            dict: {asin: embedding_vector}
        """
        if self.model is None:
            self.load_model()
            
        # æ‰«æå›¾åƒæ–‡ä»¶
        print("æ­£åœ¨æ‰«æå›¾åƒæ–‡ä»¶...")
        image_paths = []
        asins = []
        
        for filename in os.listdir(image_dir):
            if filename.lower().endswith(('.jpg', '.jpeg', '.png')):
                asin = filename.split('.')[0]  # å»æ‰æ‰©å±•åå¾—åˆ°ASIN
                image_path = os.path.join(image_dir, filename)
                image_paths.append(image_path)
                asins.append(asin)
        
        if not image_paths:
            print("âŒ æœªæ‰¾åˆ°ä»»ä½•å›¾åƒæ–‡ä»¶")
            return {}
            
        print(f"ğŸ“ æ‰¾åˆ° {len(image_paths)} ä¸ªå›¾åƒæ–‡ä»¶")
        
        # åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
        dataset = ImageDataset(image_paths, asins, self.preprocess)
        dataloader = DataLoader(
            dataset, 
            batch_size=batch_size, 
            shuffle=False, 
            num_workers=num_workers,
            pin_memory=True
        )
        
        # æ‰¹é‡ç”ŸæˆåµŒå…¥
        embeddings = {}
        failed_count = 0
        failed_images = []  # è®°å½•å¤±è´¥çš„å›¾åƒ
        
        self.model.eval()
        with torch.no_grad():
            progress_bar = tqdm(dataloader, desc="ç”Ÿæˆå›¾åƒåµŒå…¥")
            
            for batch_images, batch_asins, batch_success in progress_bar:
                batch_images = batch_images.to(self.device, non_blocking=True)
                
                # ç”Ÿæˆå›¾åƒåµŒå…¥
                if self.model_type == 'clip':
                    image_features = self.model.encode_image(batch_images)
                    # å½’ä¸€åŒ–åµŒå…¥å‘é‡
                    image_features = image_features / image_features.norm(dim=-1, keepdim=True)
                
                # è½¬æ¢ä¸ºCPU numpyæ•°ç»„å¹¶ä¿å­˜
                image_features_cpu = image_features.cpu().numpy()
                
                for i, (asin, success) in enumerate(zip(batch_asins, batch_success)):
                    if success:
                        embeddings[asin] = image_features_cpu[i]
                    else:
                        failed_count += 1
                        failed_images.append(asin)  # è®°å½•å¤±è´¥çš„ASIN
                
                # æ›´æ–°è¿›åº¦æ¡ä¿¡æ¯
                progress_bar.set_postfix({
                    ' æˆåŠŸ': len(embeddings),
                    'å¤±è´¥': failed_count,
                    'GPUå†…å­˜': f"{torch.cuda.memory_allocated() / 1024**3:.1f}GB"
                })
                
                # å®šæœŸæ¸…ç†GPUå†…å­˜
                if len(embeddings) % (batch_size * 10) == 0:
                    torch.cuda.empty_cache()
        
        print(f"\nâœ… åµŒå…¥ç”Ÿæˆå®Œæˆ!")
        print(f"  - æˆåŠŸç”Ÿæˆ: {len(embeddings)} ä¸ª")
        print(f"  - å¤±è´¥: {failed_count} ä¸ª")
        print(f"  - åµŒå…¥ç»´åº¦: {list(embeddings.values())[0].shape if embeddings else 'N/A'}")
        
        # å¦‚æœæœ‰å¤±è´¥çš„å›¾åƒï¼Œè®°å½•åˆ°æ–‡ä»¶
        if failed_images:
            failed_log_file = os.path.join(os.path.dirname(image_dir), 'failed_image_embeddings.txt')
            with open(failed_log_file, 'w') as f:
                f.write("å¤±è´¥ç”ŸæˆåµŒå…¥çš„å›¾åƒåˆ—è¡¨:\n")
                f.write("=" * 40 + "\n")
                for asin in failed_images:
                    f.write(f"{asin}\n")
            print(f"  - å¤±è´¥å›¾åƒåˆ—è¡¨å·²ä¿å­˜åˆ°: {failed_log_file}")
        
        return embeddings

def map_asins_to_item_ids(embeddings, mapping_file):
    """å°†ASINé”®è½¬æ¢ä¸ºitem_idé”®ï¼Œç”¨äºä¸è®­ç»ƒä»£ç å…¼å®¹"""
    if not os.path.exists(mapping_file):
        print(f"âš ï¸  æ˜ å°„æ–‡ä»¶ä¸å­˜åœ¨: {mapping_file}")
        print("   å°†ä½¿ç”¨ASINä½œä¸ºé”®è¿”å›åµŒå…¥")
        return embeddings
    
    print("æ­£åœ¨åŠ è½½ASINåˆ°item_idçš„æ˜ å°„...")
    with open(mapping_file, 'rb') as f:
        asin_to_itemid = pickle.load(f)
    
    # è½¬æ¢é”®ä»ASINåˆ°item_id
    itemid_embeddings = {}
    missing_count = 0
    
    for asin, embedding in embeddings.items():
        if asin in asin_to_itemid:
            item_id = asin_to_itemid[asin]
            itemid_embeddings[item_id] = embedding
        else:
            missing_count += 1
    
    print(f"âœ… é”®è½¬æ¢å®Œæˆ:")
    print(f"  - æˆåŠŸæ˜ å°„: {len(itemid_embeddings)} ä¸ª")
    print(f"  - æ˜ å°„ç¼ºå¤±: {missing_count} ä¸ª")
    
    return itemid_embeddings

def main():
    parser = argparse.ArgumentParser(description='å›¾åƒåµŒå…¥ç”Ÿæˆå™¨')
    parser.add_argument('--model_type', type=str, default='clip', 
                       choices=['clip'], help='è§†è§‰æ¨¡å‹ç±»å‹')
    parser.add_argument('--input_dir', type=str, required=True,
                       help='è¾“å…¥å›¾åƒç›®å½•')
    parser.add_argument('--output_file', type=str, required=True,
                       help='è¾“å‡ºåµŒå…¥æ–‡ä»¶è·¯å¾„ (.npy)')
    parser.add_argument('--batch_size', type=int, default=32,
                       help='æ‰¹å¤„ç†å¤§å° (4090å»ºè®®32-64)')
    parser.add_argument('--num_workers', type=int, default=4,
                       help='æ•°æ®åŠ è½½å™¨å·¥ä½œçº¿ç¨‹æ•°')
    parser.add_argument('--device', type=str, default='cuda',
                       help='è®¡ç®—è®¾å¤‡')
    parser.add_argument('--use_item_id_keys', action='store_true',
                       help='ä½¿ç”¨item_idä½œä¸ºé”® (ç”¨äºè®­ç»ƒå…¼å®¹æ€§)')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥è¾“å…¥ç›®å½•
    if not os.path.exists(args.input_dir):
        print(f"âŒ è¾“å…¥ç›®å½•ä¸å­˜åœ¨: {args.input_dir}")
        return
    
    # æ£€æŸ¥CUDAå¯ç”¨æ€§
    if args.device == 'cuda' and not torch.cuda.is_available():
        print("âš ï¸  CUDAä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPU (é€Ÿåº¦è¾ƒæ…¢)")
        args.device = 'cpu'
    
    print("ğŸš€ å›¾åƒåµŒå…¥ç”Ÿæˆå™¨å¯åŠ¨")
    print("=" * 50)
    print(f"æ¨¡å‹ç±»å‹: {args.model_type}")
    print(f"è¾“å…¥ç›®å½•: {args.input_dir}")
    print(f"è¾“å‡ºæ–‡ä»¶: {args.output_file}")
    print(f"æ‰¹å¤„ç†å¤§å°: {args.batch_size}")
    print(f"è®¡ç®—è®¾å¤‡: {args.device}")
    if args.device == 'cuda':
        print(f"GPUå‹å·: {torch.cuda.get_device_name()}")
        print(f"GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
    print("=" * 50)
    
    # åˆ›å»ºç”Ÿæˆå™¨å¹¶ç”ŸæˆåµŒå…¥
    generator = ImageEmbeddingGenerator(args.model_type, args.device)
    embeddings = generator.generate_embeddings(
        args.input_dir, 
        args.batch_size, 
        args.num_workers
    )
    
    if not embeddings:
        print("âŒ æœªç”Ÿæˆä»»ä½•åµŒå…¥ï¼Œç¨‹åºé€€å‡º")
        return
    
    # å¦‚æœéœ€è¦ï¼Œè½¬æ¢ä¸ºitem_idé”®
    if args.use_item_id_keys:
        mapping_file = os.path.join(args.input_dir, 'asin_to_itemid_mapping.pkl')
        embeddings = map_asins_to_item_ids(embeddings, mapping_file)
    
    # ä¿å­˜åµŒå…¥æ–‡ä»¶
    print(f"\nğŸ’¾ æ­£åœ¨ä¿å­˜åµŒå…¥åˆ°: {args.output_file}")
    os.makedirs(os.path.dirname(args.output_file), exist_ok=True)
    np.save(args.output_file, embeddings, allow_pickle=True)
    
    print("ğŸ‰ å›¾åƒåµŒå…¥ç”Ÿæˆå®Œæˆ!")
    print(f"ğŸ“Š æœ€ç»ˆç»Ÿè®¡:")
    print(f"  - åµŒå…¥æ•°é‡: {len(embeddings)}")
    print(f"  - æ–‡ä»¶å¤§å°: {os.path.getsize(args.output_file) / 1024**2:.1f} MB")
    
    # å†…å­˜æ¸…ç†
    torch.cuda.empty_cache()
    gc.collect()

if __name__ == '__main__':
    main()
