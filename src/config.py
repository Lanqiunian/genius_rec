# src/config.py (é‡æ„ç‰ˆ)

import torch
from pathlib import Path

def get_config():

    ROOT_DIR = Path(__file__).parent.parent 
    
    config = {
        # =================================================================
        # 1. é€šç”¨ä¸è·¯å¾„é…ç½® (General & Path Config)
        # =================================================================
        "data": {
            "data_dir": ROOT_DIR / "data",
            "processed_data_dir": ROOT_DIR / "data" / "processed",
            "log_dir": ROOT_DIR / "logs",
            "checkpoint_dir": ROOT_DIR / "checkpoints",
            
            "train_file": ROOT_DIR / "data" / "processed" / "train.parquet",
            "validation_file": ROOT_DIR / "data" / "processed" / "validation.parquet",
            "test_file": ROOT_DIR / "data" / "processed" / "test.parquet",
            "id_maps_file": ROOT_DIR / "data" / "processed" / "id_maps.pkl",
        },
        
        "k_core": 5,
        "min_seq_len": 5,
        "device": "cuda" if torch.cuda.is_available() else "cpu",
        "seed": 42,
        "pad_token_id": 0,
        "sos_token_id": 1,
        "eos_token_id": 2,
        "mask_token_id": 3,
        "num_special_tokens": 4,
        
        # =================================================================
        # 2. æ¨¡å‹è¶…å‚æ•°é…ç½® (Model Hyperparameters)
        # =================================================================
        "encoder_model": {
            "max_len": 50,
            "embedding_dim": 64,         # å»ºè®®: 128 æˆ– 256
            "linear_hidden_dim": 16,     # dv, å»ºè®®: 32 æˆ– 64
            "attention_dim": 16,         # dqk, å»ºè®®: 32 æˆ– 64
            "num_layers": 4,
            "num_heads": 4,
            "dropout": 0.1,
        },

        "decoder_model": {
            "max_seq_len": 50,
            "embedding_dim": 64,         # å»ºè®®: 128 æˆ– 256
            "num_layers": 4,
            "num_heads": 4,
            "ffn_hidden_dim": 64 * 4,    # å»ºè®®éš embedding_dim è°ƒæ•´, e.g., 128 * 4
            "dropout_ratio": 0.2,        # å»ºè®®: 0.1 æˆ– 0.2
        },

        # =================================================================
        # 3. è®­ç»ƒé˜¶æ®µé…ç½® (Training Phase Config)
        # =================================================================
        
        # --- é˜¶æ®µä¸€: ç¼–ç å™¨é¢„è®­ç»ƒé…ç½® ---
        "pretrain": {
            "log_file": "pretrain_encoder.log",
            "num_epochs": 501,         
            "batch_size": 256,         
            "learning_rate": 1e-3,
            "weight_decay": 0.1,
            "early_stopping_patience": 20,
            "num_workers": 10,
            "num_neg_samples": 512, # è´Ÿé‡‡æ ·æ•°é‡
            "temperature": 0.05,    # Sampled Softmaxæ¸©åº¦
        },
        
        # --- é˜¶æ®µäºŒ: Encoder-Decoder å¾®è°ƒé…ç½® ---
        "finetune": {
            "log_file": "finetune_genius_rec.log",
            "num_epochs": 50,
            "batch_size": 16,
            "learning_rate": {
                "decoder_lr": 1e-4,  # è§£ç å™¨å­¦ä¹ ç‡, å»ºè®®: 3e-4 æˆ– 1e-4
                "encoder_lr": 5e-6,  # ä¿æŒä¸å˜ï¼Œç”¨äºç²¾è°ƒ
                "embedding_lr": 1e-4, # åµŒå…¥å±‚å­¦ä¹ ç‡, å»ºè®®: 1e-4 æˆ– 3e-4
                "gate_lr": 1e-4      # é—¨æ§ç½‘ç»œå­¦ä¹ ç‡
            },
            "balancing_loss_alpha": 0.1, # è´Ÿè½½å‡è¡¡æŸå¤±çš„ç³»æ•°, å»ºè®®: 0.01 æˆ– 0.05
            "label_smoothing": 0,
            "warmup_steps": 1000,
            "weight_decay": 0.01,    
            "early_stopping_patience": 4,
            "num_workers": 10,

            "use_stochastic_length": False,
            "stochastic_threshold": 20,
            "stochastic_prob": 0.5,
              
        },
                
        # =================================================================
        # 4. è¯„ä¼°å‚æ•° (Evaluation Config)
        # =================================================================
        "evaluation": {
            "top_k": 10,
        },
        
        # =================================================================
        # 5. ä¸“å®¶ç³»ç»Ÿé…ç½® (Expert System Config) 
        # =================================================================
        "expert_system": {
            # ä¸“å®¶å¯ç”¨å¼€å…³
            "experts": {
                "behavior_expert": True,     # è¡Œä¸ºä¸“å®¶ï¼ˆåŸºäºç”¨æˆ·åºåˆ—è¡Œä¸ºï¼‰
                "content_expert": True,      # å†…å®¹ä¸“å®¶ï¼ˆåŸºäºæ–‡æœ¬åµŒå…¥ï¼‰
                "image_expert": True,        # å›¾åƒä¸“å®¶ï¼ˆåŸºäºä¹¦å°é¢ï¼‰ğŸ¨ å¯ç”¨è§†è§‰ä¸“å®¶ï¼
            },
            
            # é—¨æ§ç½‘ç»œé…ç½®
            "gate_config": {
                "gate_type": "mlp",       # é—¨æ§ç±»å‹ï¼š'simple'(åŸå§‹), 'mlp'(æ–°å¢)
                "gate_hidden_dim": 64,       # MLPé—¨æ§çš„éšè—å±‚ç»´åº¦ï¼ˆä»…gate_type='mlp'æ—¶ä½¿ç”¨ï¼‰
                "temperature": 1.0,          # softmaxæ¸©åº¦å‚æ•°ï¼ˆé¢„ç•™ï¼‰
                "noise_epsilon": 0.8,         # é—¨æ§ç½‘ç»œå™ªå£°å‚æ•°ï¼Œç”¨äºå¯¹æŠ—ä¸“å®¶æåŒ–
            },
            
            # å†…å®¹ä¸“å®¶é…ç½®
            "content_expert": {
                "attention_heads": 4,        # äº¤å‰æ³¨æ„åŠ›å¤´æ•°
                "use_cross_attention": True, # æ˜¯å¦ä½¿ç”¨äº¤å‰æ³¨æ„åŠ›
                "text_projection_type": "mlp",     # æ–‡æœ¬æŠ•å½±ç±»å‹ï¼š'simple'ï¼ˆåŸå§‹ï¼‰, 'mlp'
                "text_embedding_dim": 768,   # æ–‡æœ¬åµŒå…¥ç»´åº¦
                "trainable_embeddings": False 
            },
            
            # å›¾åƒä¸“å®¶é…ç½®
            "image_expert": {
                "attention_heads": 4,        # äº¤å‰æ³¨æ„åŠ›å¤´æ•°  
                "use_cross_attention": True, # æ˜¯å¦ä½¿ç”¨äº¤å‰æ³¨æ„åŠ›
                "image_embedding_dim": 512,  # å›¾åƒåµŒå…¥ç»´åº¦ï¼ˆCLIP ViT-B/32ï¼‰
                "image_encoder": "clip",     # å›¾åƒç¼–ç å™¨ç±»å‹
                "use_adaptive_pooling": True, # ä½¿ç”¨è‡ªé€‚åº”æ± åŒ–é€‚é…ä¸åŒç»´åº¦
                "image_projection_type": "mlp", # å›¾åƒæŠ•å½±ç±»å‹ï¼š'simple', 'mlp'
                "trainable_embeddings": False
            }
        }
    }
    
    # ğŸ”§ æ–°å¢ï¼šé…ç½®éªŒè¯
    special_ids = [config['pad_token_id'], config['sos_token_id'], 
                   config['eos_token_id'], config['mask_token_id']]
    if len(set(special_ids)) != len(special_ids):
        raise ValueError("Special token IDs must be unique!")
    if config['pad_token_id'] != 0:
        raise ValueError("pad_token_id must be 0 for PyTorch compatibility!")
    
    return config