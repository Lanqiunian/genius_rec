# src/evaluation.py
"""
è¯„ä¼°ç›¸å…³çš„å‡½æ•°æ¨¡å—
åŒ…å«éªŒè¯é›†å’Œæµ‹è¯•é›†çš„è¯„ä¼°å‡½æ•°
"""

import math
import numpy as np
import torch
import torch.nn.functional as F
from tqdm import tqdm
from torch.utils.data import Dataset
import pandas as pd


class ValidationDataset(Dataset):
    """
    ç”¨äºæ’åºæŒ‡æ ‡è¯„ä¼°çš„æ•°æ®é›†ï¼š
    - åªä»éªŒè¯/æµ‹è¯•é›†ä¸­å–æ•°æ®
    - ä½¿ç”¨Leave-One-Outæ–¹å¼è¯„ä¼°
    - ç¡®ä¿è®­ç»ƒæ—¶æ²¡æœ‰è§è¿‡å®Œæ•´åºåˆ—
    """
    def __init__(self, data_path, max_len, pad_token_id=0):
        self.data = pd.read_parquet(data_path)
        self.max_len = max_len
        self.pad_token_id = pad_token_id

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        full_seq = self.data.iloc[idx]['history']
        # Leave-One-Out: æœ€åä¸€ä¸ªä½œä¸ºç›®æ ‡ï¼Œå…¶ä½™ä½œä¸ºè¾“å…¥
        ground_truth_item = full_seq[-1]
        input_seq = full_seq[:-1]
        
        # æˆªæ–­å’Œå¡«å……
        if len(input_seq) > self.max_len:
            input_seq = input_seq[-self.max_len:]
        
        padded_input_seq = np.full(self.max_len, self.pad_token_id, dtype=np.int64)
        padded_input_seq[-len(input_seq):] = input_seq
        
        return {
            'input_ids': torch.tensor(padded_input_seq, dtype=torch.long),
            'ground_truth': torch.tensor(ground_truth_item, dtype=torch.long)
        }


def compute_ranking_metrics(user_embeddings, all_item_embeddings, target_item_ids, k=10):
    """
    è®¡ç®—HR@Kå’ŒNDCG@KæŒ‡æ ‡
    """
    batch_size = user_embeddings.size(0)
    
    # L2å½’ä¸€åŒ–
    user_embeddings = F.normalize(user_embeddings, p=2, dim=1)
    all_item_embeddings = F.normalize(all_item_embeddings, p=2, dim=1)
    
    # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
    scores = torch.matmul(user_embeddings, all_item_embeddings.t())
    
    # æ’åº
    _, sorted_indices = torch.sort(scores, dim=1, descending=True)
    
    hr_list, ndcg_list = [], []
    
    for i in range(batch_size):
        target_id = target_item_ids[i].item()
        if target_id == 0: 
            continue

        target_idx = target_id - 1  # IDåˆ°ç´¢å¼•çš„è½¬æ¢
        target_rank_positions = (sorted_indices[i] == target_idx).nonzero(as_tuple=True)[0]
        
        hr, ndcg = 0.0, 0.0
        if len(target_rank_positions) > 0:
            rank = target_rank_positions[0].item() + 1
            if rank <= k:
                hr = 1.0
                ndcg = 1.0 / np.log2(rank + 1)
        
        hr_list.append(hr)
        ndcg_list.append(ndcg)
    
    return hr_list, ndcg_list


def evaluate_model_validation(model, val_loader, criterion, device, epoch, num_epochs, pad_token_id):
    """
    éªŒè¯é›†è¯„ä¼°ï¼šåªè®¡ç®—losså’Œpplï¼Œç”¨äºæ—©åœå’Œæ¨¡å‹é€‰æ‹©
    """
    model.eval()
    
    total_loss_tokens = 0.0
    total_tokens = 0
    total_gate_weights = None
    total_valid_batches = 0

    progress_bar = tqdm(val_loader, desc=f"Epoch {epoch+1}/{num_epochs} [Validation]")

    with torch.no_grad():
        for batch in progress_bar:
            source_ids = batch['source_ids'].to(device)
            decoder_input_ids = batch['decoder_input_ids'].to(device)
            labels = batch['labels'].to(device)
            source_padding_mask = (source_ids == pad_token_id)

            logits, gate_weights = model(source_ids, decoder_input_ids, source_padding_mask, return_weights=True)
            
            # ä¿®æ­£ï¼šä½¿ç”¨ä¼ ç»Ÿçš„CrossEntropyLossè°ƒç”¨æ–¹å¼
            loss = criterion(logits.view(-1, logits.size(-1)), labels.view(-1))
            
            valid_tokens = (labels.view(-1) != pad_token_id).sum().item()
            total_loss_tokens += loss.item() * valid_tokens
            total_tokens += valid_tokens

            # ç´¯è®¡é—¨æ§æƒé‡ï¼ˆåŠ¨æ€æ”¯æŒå¤šä¸ªä¸“å®¶ï¼‰
            non_padding_mask = (decoder_input_ids != pad_token_id)
            if gate_weights.size(-1) > 0:  # ç¡®ä¿æœ‰ä¸“å®¶
                masked_gate_weights = gate_weights[non_padding_mask]  # (N, num_experts)
                if masked_gate_weights.numel() > 0:
                    if total_gate_weights is None:
                        total_gate_weights = masked_gate_weights.mean(dim=0)  # (num_experts,)
                        total_valid_batches = 1
                    else:
                        total_gate_weights += masked_gate_weights.mean(dim=0)
                        total_valid_batches += 1

            progress_bar.set_postfix(loss=loss.item())

    avg_loss = total_loss_tokens / total_tokens if total_tokens > 0 else 0
    perplexity = math.exp(avg_loss) if avg_loss < 20 else float('inf')
    
    # è®¡ç®—å¹³å‡é—¨æ§æƒé‡
    avg_gate_weights = total_gate_weights / total_valid_batches if total_valid_batches > 0 else None
    
    result = {
        'val_loss': avg_loss,
        'val_ppl': perplexity,
    }
    
    # åŠ¨æ€æ·»åŠ ä¸“å®¶æƒé‡ä¿¡æ¯
    if avg_gate_weights is not None:
        enabled_experts = [k for k, v in model.decoder.expert_config["experts"].items() if v]
        for i, expert_name in enumerate(enabled_experts):
            if i < len(avg_gate_weights):
                result[f'avg_{expert_name}_weight'] = avg_gate_weights[i].item()
    
    return result


def evaluate_model_test(model, test_loader, device, item_num, top_k=10):
    """
    æµ‹è¯•é›†è¯„ä¼°ï¼šåªè®¡ç®—æ’åºæŒ‡æ ‡ï¼Œè®­ç»ƒç»“æŸåè°ƒç”¨ä¸€æ¬¡
    
    ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨ä¸HSTUå®Œå…¨ç›¸åŒçš„è¯„ä¼°é€»è¾‘
    """
    model.eval()
    hr_total = 0.0
    ndcg_total = 0.0
    total_samples = 0
    
    with torch.no_grad():
        # é¢„å…ˆè®¡ç®—æ‰€æœ‰ç‰©å“åµŒå…¥ï¼Œé¿å…é‡å¤è®¡ç®—
        all_item_ids = torch.arange(1, item_num, device=device)
        all_item_embeddings = model.encoder.item_embedding(all_item_ids)
        
        progress_bar = tqdm(test_loader, desc="Test Set Evaluation - HSTU Style")
        
        for batch in progress_bar:
            input_ids = batch['input_ids'].to(device)
            ground_truth_ids = batch['ground_truth'].to(device)

            # è·å–ç”¨æˆ·åµŒå…¥
            encoder_output = model.encoder(input_ids)
            user_embeddings = encoder_output[:, -1, :]  # å–æœ€åä¸€ä¸ªä½ç½®
            
            # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨ä¸HSTUå®Œå…¨ç›¸åŒçš„è¯„ä¼°é€»è¾‘
            batch_size = input_ids.size(0)
            hr, ndcg = get_metrics_hstu_style(
                user_embeddings,
                all_item_embeddings,
                ground_truth_ids,
                k=top_k
            )
            
            hr_total += hr * batch_size
            ndcg_total += ndcg * batch_size
            total_samples += batch_size
    
    avg_hr = hr_total / total_samples if total_samples > 0 else 0.0
    avg_ndcg = ndcg_total / total_samples if total_samples > 0 else 0.0
    
    return {
        'test_hr': avg_hr,
        'test_ndcg': avg_ndcg,
        'evaluated_samples': total_samples
    }


def evaluate_model_validation_with_ranking(model, val_loader, criterion, device, epoch, num_epochs, pad_token_id, num_candidates=1000, top_k=10):
    """
    éªŒè¯é›†è¯„ä¼°ï¼šè®¡ç®—lossã€pplå’Œæ’åºæŒ‡æ ‡
    
    âš ï¸ å®Œå…¨é‡æ„ï¼šä½¿ç”¨ä¸HSTU/baselineå®Œå…¨ç›¸åŒçš„è¯„ä¼°é€»è¾‘
    
    Args:
        num_candidates: ä¿ç•™å‚æ•°ä»¥ä¿æŒæ¥å£å…¼å®¹æ€§ï¼Œä½†å®é™…ä½¿ç”¨å…¨é‡è¯„ä¼°
    """
    model.eval()
    
    total_loss_tokens = 0.0
    total_tokens = 0
    total_gate_weights = None
    total_valid_batches = 0
    
    # ğŸš€ æ’åºæŒ‡æ ‡è®¡ç®—ï¼ˆä½¿ç”¨å®Œå…¨ç›¸åŒçš„HSTUè®¡ç®—æ–¹æ³•ï¼‰
    hr_total = 0.0
    ndcg_total = 0.0
    total_samples = 0
    
    # ğŸ”§ é¢„å…ˆè®¡ç®—æ‰€æœ‰ç‰©å“åµŒå…¥ï¼ˆä¸åŒ…å«padding token 0ï¼‰- ä¸HSTUå®Œå…¨ä¸€è‡´
    with torch.no_grad():
        # è·å–æ‰€æœ‰æœ‰æ•ˆç‰©å“çš„åµŒå…¥ï¼ˆIDä»1å¼€å§‹ï¼Œè·³è¿‡padding token 0ï¼‰
        item_num = model.encoder.item_embedding.num_embeddings
        all_item_ids = torch.arange(1, item_num, device=device)
        all_item_embeddings = model.encoder.item_embedding(all_item_ids)  # [num_items-1, embed_dim]
    
    progress_bar = tqdm(val_loader, desc=f"Epoch {epoch+1}/{num_epochs} [Validation - Full Eval]")

    with torch.no_grad():
        for batch in progress_bar:
            source_ids = batch['source_ids'].to(device)
            decoder_input_ids = batch['decoder_input_ids'].to(device)
            labels = batch['labels'].to(device)
            source_padding_mask = (source_ids == pad_token_id)

            logits, gate_weights = model(source_ids, decoder_input_ids, source_padding_mask, return_weights=True)
            
            # è®¡ç®—losså’Œppl
            loss = criterion(logits.view(-1, logits.size(-1)), labels.view(-1))
            
            valid_tokens = (labels.view(-1) != pad_token_id).sum().item()
            total_loss_tokens += loss.item() * valid_tokens
            total_tokens += valid_tokens

            # ç´¯è®¡é—¨æ§æƒé‡
            non_padding_mask = (decoder_input_ids != pad_token_id)
            if gate_weights.size(-1) > 0:
                masked_gate_weights = gate_weights[non_padding_mask]
                if masked_gate_weights.numel() > 0:
                    if total_gate_weights is None:
                        total_gate_weights = masked_gate_weights.mean(dim=0)
                        total_valid_batches = 1
                    else:
                        total_gate_weights += masked_gate_weights.mean(dim=0)
                        total_valid_batches += 1
            
            # ğŸš€ å…¨é¢é‡æ„ï¼šæ’åºæŒ‡æ ‡è®¡ç®— - å®Œå…¨é‡‡ç”¨HSTUçš„è¯„ä¼°æ–¹æ³•
            # è·å–ç”¨æˆ·è¡¨ç¤º
            batch_size = source_ids.size(0)
            # å¯¹æ•´ä¸ªæ‰¹æ¬¡è·å–ç¼–ç å™¨è¾“å‡º
            encoder_outputs = model.encoder(source_ids)  # [B, L, D]
            user_embeddings = encoder_outputs[:, -1, :]  # [B, D] - å–æ¯ä¸ªåºåˆ—çš„æœ€åä¸€ä¸ªä½ç½®ä½œä¸ºç”¨æˆ·è¡¨ç¤º
            
            # ä»æ ‡ç­¾ä¸­æå–çœŸå®ç›®æ ‡ç‰©å“ID
            target_item_ids = []
            for i in range(batch_size):
                valid_positions = (labels[i] != pad_token_id).nonzero(as_tuple=True)[0]
                if len(valid_positions) == 0:
                    # å¦‚æœæ²¡æœ‰æœ‰æ•ˆæ ‡ç­¾ï¼Œç”¨0å¡«å……ï¼ˆåé¢ä¼šè·³è¿‡ï¼‰
                    target_item_ids.append(0)
                else:
                    # å–ç¬¬ä¸€ä¸ªépaddingçš„æ ‡ç­¾ä½œä¸ºç›®æ ‡
                    target_item_ids.append(labels[i, valid_positions[0]].item())
            
            target_item_ids = torch.tensor(target_item_ids, device=device)
            
            # å®Œå…¨é‡‡ç”¨HSTUçš„è®¡ç®—æ–¹å¼
            hr, ndcg = get_metrics_hstu_style(
                user_embeddings,
                all_item_embeddings,
                target_item_ids,
                k=top_k
            )
            
            hr_total += hr * batch_size
            ndcg_total += ndcg * batch_size
            total_samples += batch_size

            progress_bar.set_postfix(loss=loss.item())
        
        # ğŸ”§ æ–°å¢ï¼šæ¸…ç†GPUæ˜¾å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

    # è®¡ç®—æœ€ç»ˆæŒ‡æ ‡
    avg_loss = total_loss_tokens / total_tokens if total_tokens > 0 else 0
    perplexity = math.exp(avg_loss) if avg_loss < 20 else float('inf')
    avg_gate_weights = total_gate_weights / total_valid_batches if total_valid_batches > 0 else None
    
    # ğŸš€ ä¿®å¤ï¼šæ’åºæŒ‡æ ‡ - å®Œå…¨å¯¹é½HSTUè®¡ç®—æ–¹å¼
    avg_hr = hr_total / total_samples if total_samples > 0 else 0.0
    avg_ndcg = ndcg_total / total_samples if total_samples > 0 else 0.0
    
    result = {
        'val_loss': avg_loss,
        'val_ppl': perplexity,
        'val_hr': avg_hr,          # ä¸HSTUå®Œå…¨å¯¹é½çš„HR@K
        'val_ndcg': avg_ndcg,      # ä¸HSTUå®Œå…¨å¯¹é½çš„NDCG@K
        'evaluated_samples': total_samples
    }
    
    # åŠ¨æ€æ·»åŠ ä¸“å®¶æƒé‡ä¿¡æ¯
    if avg_gate_weights is not None:
        enabled_experts = [k for k, v in model.decoder.expert_config["experts"].items() if v]
        for i, expert_name in enumerate(enabled_experts):
            if i < len(avg_gate_weights):
                result[f'avg_{expert_name}_weight'] = avg_gate_weights[i].item()
    
    return result


def get_metrics_hstu_style(user_embeddings, all_item_embeddings, target_item_ids, k=10):
    """
    å…¨é‡è¯„ä¼°ï¼šè®¡ç®—ç”¨æˆ·åµŒå…¥ä¸æ‰€æœ‰ç‰©å“åµŒå…¥çš„ç›¸ä¼¼åº¦ï¼ˆä¸HSTUå®Œå…¨ä¸€è‡´ï¼‰
    
    Args:
        user_embeddings: ç”¨æˆ·åµŒå…¥ [batch_size, embed_dim]
        all_item_embeddings: æ‰€æœ‰ç‰©å“çš„åµŒå…¥ [num_items-1, embed_dim]
        target_item_ids: ç›®æ ‡ç‰©å“ID [batch_size]
        k: æ¨èåˆ—è¡¨é•¿åº¦
        
    Returns:
        hr: HR@k
        ndcg: NDCG@k
    """
    batch_size = user_embeddings.size(0)
    
    # L2å½’ä¸€åŒ–ï¼ˆå¯¹é½HSTUå®ç°ï¼‰
    user_embeddings = F.normalize(user_embeddings, p=2, dim=1)
    all_item_embeddings = F.normalize(all_item_embeddings, p=2, dim=1)
    
    # è®¡ç®—ç”¨æˆ·ä¸æ‰€æœ‰ç‰©å“çš„ç›¸ä¼¼åº¦ [batch_size, num_items-1]
    scores = torch.matmul(user_embeddings, all_item_embeddings.t())
    
    # æ’åºè·å–æ’åï¼ˆé™åºï¼‰
    _, sorted_indices = torch.sort(scores, dim=1, descending=True)
    
    hr_count = 0
    ndcg_sum = 0.0
    valid_samples = 0
    
    for i in range(batch_size):
        target_id = target_item_ids[i].item()
        if target_id == 0:
            continue
        valid_samples += 1
            
        target_idx = target_id - 1  # IDåˆ°ç´¢å¼•çš„è½¬æ¢
        target_rank_positions = (sorted_indices[i] == target_idx).nonzero(as_tuple=True)[0]
        
        if len(target_rank_positions) > 0:
            rank = target_rank_positions[0].item() + 1
            
            if rank <= k:
                hr_count += 1
                ndcg_sum += 1.0 / np.log2(rank + 1)
    
    # æ³¨æ„ï¼šè¿™é‡Œä½¿ç”¨æœ‰æ•ˆæ ·æœ¬æ•°è€Œéæ€»æ‰¹æ¬¡å¤§å°
    hr = hr_count / valid_samples if valid_samples > 0 else 0
    ndcg = ndcg_sum / valid_samples if valid_samples > 0 else 0
    
    return hr, ndcg
