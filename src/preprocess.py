# preprocess.py (é‡æ„ç‰ˆ)

import pandas as pd
import gzip
from tqdm import tqdm
import pickle
import json
from pathlib import Path
from src.config import get_config

def parse_jsonl_to_df(file_path):
    """è§£æJSONLå¹¶ç›´æ¥è¿”å›åªå«æ‰€éœ€åˆ—çš„DataFrameï¼Œä»¥èŠ‚çœå†…å­˜"""
    records = []
    with gzip.open(file_path, 'rt', encoding='utf-8') as f:
        for line in tqdm(f, desc=f"è§£æ {Path(file_path).name}"):
            try:
                data = json.loads(line)
                # åªæå–æˆ‘ä»¬éœ€è¦çš„å­—æ®µ
                records.append({
                    'user_id': data.get('user_id'),
                    'item_id': data.get('asin'),
                    'timestamp': data.get('timestamp')
                })
            except (json.JSONDecodeError, AttributeError):
                continue
    return pd.DataFrame(records)

def main():
    """å®Œå…¨å¯¹é½å®˜æ–¹ä»£ç åº“ä¸­AmazonDataProcessorçš„é¢„å¤„ç†é€»è¾‘"""
    config = get_config()
    
    # ä½¿ç”¨é…ç½®ä¸­çš„è·¯å¾„
    DATA_DIR = config['data']['data_dir']
    PROCESSED_DATA_DIR = config['data']['processed_data_dir']
    
    # è¾“å…¥æ–‡ä»¶
    REVIEW_FILE = DATA_DIR / "Books.jsonl.gz" 
    
    # è¾“å‡ºæ–‡ä»¶
    TRAIN_FILE = config['data']['train_file']
    VALIDATION_FILE = config['data']['validation_file']
    TEST_FILE = config['data']['test_file']
    ID_MAPS_FILE = config['data']['id_maps_file']
    
    # é…ç½®å‚æ•°
    K_CORE = config['k_core']
    MIN_SEQ_LEN = config['min_seq_len']
    SEED = config['seed']
    
    PROCESSED_DATA_DIR.mkdir(parents=True, exist_ok=True)
    
    print("--- 1. åŠ è½½åŸå§‹è¯„è®ºæ•°æ® ---")
    ratings = parse_jsonl_to_df(REVIEW_FILE)
    ratings.dropna(inplace=True) # ä¸¢å¼ƒæ— æ•ˆè¡Œ
    ratings['timestamp'] = pd.to_datetime(ratings['timestamp'])
    
    print(f"åŸå§‹æ•°æ®ç‚¹: {len(ratings)}")
    print(f"åŸå§‹ç”¨æˆ·æ•°: {ratings['user_id'].nunique()}")
    print(f"åŸå§‹ç‰©å“æ•°: {ratings['item_id'].nunique()}")

    print(f"--- 2. æ‰§è¡Œ {K_CORE}-core è¿‡æ»¤ ---")
    while True:
        user_counts = ratings['user_id'].value_counts()
        item_counts = ratings['item_id'].value_counts()
        initial_rows = len(ratings)
        user_mask = user_counts[user_counts >= K_CORE].index
        item_mask = item_counts[item_counts >= K_CORE].index
        ratings = ratings[ratings['user_id'].isin(user_mask) & ratings['item_id'].isin(item_mask)]
        if initial_rows == len(ratings):
            break
            
    print(f"è¿‡æ»¤åæ•°æ®ç‚¹: {len(ratings)}")
    print(f"è¿‡æ»¤åç”¨æˆ·æ•°: {ratings['user_id'].nunique()}")
    print(f"è¿‡æ»¤åç‰©å“æ•°: {ratings['item_id'].nunique()}")

    # --- 3. ID é‡æ˜ å°„ (å¯¹é½å®˜æ–¹) ---
    print("--- 3. å°†user_idå’Œitem_idé‡æ˜ å°„ä¸ºè¿ç»­æ•´æ•° ---")
    
    # è·å–ç‰¹æ®Šæ ‡è®°é…ç½®
    pad_token_id = config['pad_token_id']
    sos_token_id = config['sos_token_id'] 
    eos_token_id = config['eos_token_id']
    mask_token_id = config['mask_token_id']
    
    # è®¡ç®—ç‰¹æ®Šæ ‡è®°çš„æ•°é‡ï¼Œä¸ºå…¶é¢„ç•™IDç©ºé—´
    special_token_ids = [pad_token_id, sos_token_id, eos_token_id, mask_token_id]
    num_special_tokens = max(special_token_ids) + 1
    
    # ğŸ”§ å®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿ç‰¹æ®Štoken IDè¿ç»­ä¸”ä»0å¼€å§‹
    expected_special_ids = list(range(len(special_token_ids)))
    if sorted(special_token_ids) != expected_special_ids:
        raise ValueError(f"ç‰¹æ®Štoken IDå¿…é¡»è¿ç»­ä¸”ä»0å¼€å§‹ï¼Œå½“å‰: {sorted(special_token_ids)}, æœŸæœ›: {expected_special_ids}")
    
    # ã€å…³é”®ä¿®æ­£ã€‘é‡æ˜ å°„åçš„IDä»num_special_tokenså¼€å§‹ï¼Œä¸ºç‰¹æ®Šæ ‡è®°ç•™å‡ºç©ºé—´
    ratings['user_id_remap'] = pd.Categorical(ratings['user_id']).codes + num_special_tokens
    ratings['item_id_remap'] = pd.Categorical(ratings['item_id']).codes + num_special_tokens
    
    # ã€å…³é”®ä¿®æ­£ã€‘ä¿å­˜æ–°çš„æ˜ å°„å…³ç³»
    user_map = {original: remap_id for original, remap_id in zip(ratings['user_id'], ratings['user_id_remap'])}
    item_map = {original: remap_id for original, remap_id in zip(ratings['item_id'], ratings['item_id_remap'])}
    
    id_maps = {
        'user_map': user_map,
        'item_map': item_map,
        'num_users': len(user_map),
        'num_items': len(item_map),
        'num_special_tokens': num_special_tokens,
        'special_tokens': {
            'pad_token_id': pad_token_id,
            'sos_token_id': sos_token_id,
            'eos_token_id': eos_token_id,
            'mask_token_id': mask_token_id
        }
    }

    print("--- 4. æŒ‰ç”¨æˆ·åˆ†ç»„ç”Ÿæˆåºåˆ— (ä½¿ç”¨é‡æ˜ å°„åçš„ID) ---")
    df_sorted = ratings.sort_values(by=['user_id_remap', 'timestamp'])
    grouped = df_sorted.groupby('user_id_remap')['item_id_remap'].apply(list)
    
    print(f"--- 5. è¿‡æ»¤æ‰é•¿åº¦å°äº {MIN_SEQ_LEN} çš„åºåˆ— ---")
    user_history = grouped[grouped.apply(len) >= MIN_SEQ_LEN].reset_index()
    user_history.rename(columns={'item_id_remap': 'history'}, inplace=True)
    
    print(f"æœ€ç»ˆæœ‰æ•ˆç”¨æˆ·æ•°: {len(user_history)}")
    
    print("--- 6. åˆ’åˆ†è®­ç»ƒ/éªŒè¯/æµ‹è¯•é›† ---")
    user_ids = user_history['user_id_remap'].unique()
    import numpy as np
    np.random.seed(SEED)
    np.random.shuffle(user_ids)
    
    train_size = int(len(user_ids) * 0.8)
    val_size = int(len(user_ids) * 0.1)
    
    train_users = user_ids[:train_size]
    val_users = user_ids[train_size : train_size + val_size]
    test_users = user_ids[train_size + val_size:]
    
    train_df = user_history[user_history['user_id_remap'].isin(train_users)]
    validation_df = user_history[user_history['user_id_remap'].isin(val_users)]
    test_df = user_history[user_history['user_id_remap'].isin(test_users)]

    print("--- 7. ä¿å­˜å¤„ç†åçš„æ•°æ® ---")
    train_df[['history']].to_parquet(TRAIN_FILE)
    validation_df[['history']].to_parquet(VALIDATION_FILE)
    test_df[['history']].to_parquet(TEST_FILE)
    with open(ID_MAPS_FILE, 'wb') as f:
        pickle.dump(id_maps, f)
    print("é¢„å¤„ç†å®Œæˆï¼")

if __name__ == '__main__':
    main()