# src/train_GeniusRec.py
import argparse
import logging
import os
import pickle
import random
import math
import pathlib
import platform

from tqdm import tqdm
from transformers import get_linear_schedule_with_warmup
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset

from src.config import get_config
from src.GeniusRec import GENIUSRecModel
from src.dataset import Seq2SeqRecDataset
# Make sure you import HSTU and GenerativeDecoder
from src.encoder.encoder import Hstu
from src.decoder.decoder import GenerativeDecoder


# # ä»å¤´å¼€å§‹è®­ç»ƒ
# python -m src.train_GeniusRec --encoder_weights_path checkpoints/hstu_encoder.pth --freeze_encoder

# # ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ
# python -m src.train_GeniusRec --resume_from checkpoints/genius_rec_moe_latest.pth --encoder_weights_path checkpoints/hstu_encoder.pth

# # è‡ªå®šä¹‰ä¿å­˜ç›®å½•
# python -m src.train_GeniusRec --save_dir my_checkpoints

# --- æ— æ•°æ®æ³„éœ²çš„è¯„ä¼°æ•°æ®é›† ---
class ValidationDataset(Dataset):
    """
    ç”¨äºæ’åºæŒ‡æ ‡è¯„ä¼°çš„æ•°æ®é›†ï¼š
    - åªä»éªŒè¯/æµ‹è¯•é›†ä¸­å–æ•°æ®
    - ä½¿ç”¨Leave-One-Outæ–¹å¼è¯„ä¼°
    - ç¡®ä¿è®­ç»ƒæ—¶æ²¡æœ‰è§è¿‡å®Œæ•´åºåˆ—
    """
    def __init__(self, data_path, max_len, pad_token_id=0):
        self.data = pd.read_parquet(data_path)
        self.max_len = max_len
        self.pad_token_id = pad_token_id

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        full_seq = self.data.iloc[idx]['history']
        # Leave-One-Out: æœ€åä¸€ä¸ªä½œä¸ºç›®æ ‡ï¼Œå…¶ä½™ä½œä¸ºè¾“å…¥
        ground_truth_item = full_seq[-1]
        input_seq = full_seq[:-1]
        
        # æˆªæ–­å’Œå¡«å……
        if len(input_seq) > self.max_len:
            input_seq = input_seq[-self.max_len:]
        
        padded_input_seq = np.full(self.max_len, self.pad_token_id, dtype=np.int64)
        padded_input_seq[-len(input_seq):] = input_seq
        
        return {
            'input_ids': torch.tensor(padded_input_seq, dtype=torch.long),
            'ground_truth': torch.tensor(ground_truth_item, dtype=torch.long)
        }

def compute_ranking_metrics(user_embeddings, all_item_embeddings, target_item_ids, k=10):
    """
    è®¡ç®—HR@Kå’ŒNDCG@KæŒ‡æ ‡
    """
    batch_size = user_embeddings.size(0)
    
    # L2å½’ä¸€åŒ–
    user_embeddings = F.normalize(user_embeddings, p=2, dim=1)
    all_item_embeddings = F.normalize(all_item_embeddings, p=2, dim=1)
    
    # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
    scores = torch.matmul(user_embeddings, all_item_embeddings.t())
    
    # æ’åº
    _, sorted_indices = torch.sort(scores, dim=1, descending=True)
    
    hr_list, ndcg_list = [], []
    
    for i in range(batch_size):
        target_id = target_item_ids[i].item()
        if target_id == 0: 
            continue

        target_idx = target_id - 1  # IDåˆ°ç´¢å¼•çš„è½¬æ¢
        target_rank_positions = (sorted_indices[i] == target_idx).nonzero(as_tuple=True)[0]
        
        hr, ndcg = 0.0, 0.0
        if len(target_rank_positions) > 0:
            rank = target_rank_positions[0].item() + 1
            if rank <= k:
                hr = 1.0
                ndcg = 1.0 / np.log2(rank + 1)
        
        hr_list.append(hr)
        ndcg_list.append(ndcg)
    
    return hr_list, ndcg_list

def train_one_epoch(model, dataloader, criterion, optimizer, scheduler, device, epoch, num_epochs, pad_token_id):
    """
    è®­ç»ƒä¸€ä¸ªepoch
    """
    model.train()
    total_loss = 0.0
    progress_bar = tqdm(dataloader, desc=f"Epoch {epoch+1}/{num_epochs} [Training]")

    for batch in progress_bar:
        source_ids = batch['source_ids'].to(device)
        decoder_input_ids = batch['decoder_input_ids'].to(device)
        labels = batch['labels'].to(device)
        source_padding_mask = (source_ids == pad_token_id)

        optimizer.zero_grad()
        
        logits = model(source_ids, decoder_input_ids, source_padding_mask, return_weights=False)

        loss = criterion(logits.view(-1, logits.size(-1)), labels.view(-1))
        
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        optimizer.step()
        scheduler.step()
        
        total_loss += loss.item()
        progress_bar.set_postfix(loss=loss.item(), avg_loss=total_loss/len(progress_bar))
        
    avg_loss = total_loss / len(dataloader)
    return avg_loss

def evaluate_model(model, val_loader, eval_loader, criterion, device, epoch, num_epochs, 
                   pad_token_id, item_num, top_k=10):
    """
    å®Œæ•´çš„æ¨¡å‹è¯„ä¼°å‡½æ•°ï¼šè®¡ç®—loss/pplå’Œæ’åºæŒ‡æ ‡
    æ¯ä¸ªepochéƒ½ä¼šè®¡ç®—æ‰€æœ‰æŒ‡æ ‡ï¼Œç¡®ä¿èƒ½åŠæ—¶å‘ç°æ€§èƒ½å˜åŒ–
    """
    model.eval()
    
    # === ç¬¬ä¸€éƒ¨åˆ†ï¼šéªŒè¯é›†ä¸Šçš„losså’Œpplè¯„ä¼° ===
    total_loss = 0.0
    total_behavior_weight = 0.0
    total_content_weight = 0.0
    total_valid_batches = 0

    progress_bar = tqdm(val_loader, desc=f"Epoch {epoch+1}/{num_epochs} [Validation Loss/PPL]")

    with torch.no_grad():
        for batch in progress_bar:
            source_ids = batch['source_ids'].to(device)
            decoder_input_ids = batch['decoder_input_ids'].to(device)
            labels = batch['labels'].to(device)
            source_padding_mask = (source_ids == pad_token_id)

            logits, gate_weights = model(source_ids, decoder_input_ids, source_padding_mask, return_weights=True)
            
            loss = criterion(logits.view(-1, logits.size(-1)), labels.view(-1))
            total_loss += loss.item()

            non_padding_mask = (decoder_input_ids != pad_token_id)
            behavior_weights = gate_weights[:, :, 0][non_padding_mask]
            content_weights = gate_weights[:, :, 1][non_padding_mask]
            
            if behavior_weights.numel() > 0:
                total_behavior_weight += behavior_weights.mean().item()
                total_content_weight += content_weights.mean().item()
                total_valid_batches += 1

            progress_bar.set_postfix(loss=loss.item())

    avg_loss = total_loss / len(val_loader)
    perplexity = math.exp(avg_loss) if avg_loss < 20 else float('inf')
    avg_behavior_w = total_behavior_weight / total_valid_batches if total_valid_batches > 0 else 0
    avg_content_w = total_content_weight / total_valid_batches if total_valid_batches > 0 else 0
    
    # === ç¬¬äºŒéƒ¨åˆ†ï¼šæ’åºæŒ‡æ ‡è¯„ä¼° ===
    all_hr_scores, all_ndcg_scores = [], []
    
    with torch.no_grad():
        # åˆ›å»ºæ‰€æœ‰ç‰©å“çš„åµŒå…¥çŸ©é˜µ
        all_item_ids = torch.arange(1, item_num, device=device)
        all_item_embeddings = model.encoder.item_embedding(all_item_ids)
        
        progress_bar = tqdm(eval_loader, desc=f"Epoch {epoch+1}/{num_epochs} [Ranking Metrics]")
        
        for batch in progress_bar:
            input_ids = batch['input_ids'].to(device)
            ground_truth_ids = batch['ground_truth'].to(device)

            # è·å–ç”¨æˆ·åµŒå…¥
            encoder_output = model.encoder(input_ids)
            user_embeddings = encoder_output[:, -1, :]  # å–æœ€åä¸€ä¸ªä½ç½®
            
            # è®¡ç®—HRå’ŒNDCG
            hr_list, ndcg_list = compute_ranking_metrics(
                user_embeddings,
                all_item_embeddings,
                ground_truth_ids,
                k=top_k
            )
            all_hr_scores.extend(hr_list)
            all_ndcg_scores.extend(ndcg_list)
    
    avg_hr = np.mean(all_hr_scores) if all_hr_scores else 0.0
    avg_ndcg = np.mean(all_ndcg_scores) if all_ndcg_scores else 0.0
    
    return {
        'val_loss': avg_loss,
        'val_ppl': perplexity,
        'avg_behavior_weight': avg_behavior_w,
        'avg_content_weight': avg_content_w,
        'hr': avg_hr,
        'ndcg': avg_ndcg,
        'evaluated_samples': len(all_hr_scores)
    }

def load_checkpoint(checkpoint_path, model, optimizer, scheduler, device):
    """
    æ£€æŸ¥ç‚¹åŠ è½½å‡½æ•°ï¼Œæ”¯æŒå®Œæ•´æ£€æŸ¥ç‚¹å’Œä»…æƒé‡çš„.pthæ–‡ä»¶
    
    Returns:
        dict: åŒ…å«æ¢å¤ä¿¡æ¯çš„å­—å…¸
    """
    logging.info(f"å°è¯•åŠ è½½æ£€æŸ¥ç‚¹: {checkpoint_path}")
    
    try:
        checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºå®Œæ•´æ£€æŸ¥ç‚¹æ ¼å¼
        if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
            # å®Œæ•´æ£€æŸ¥ç‚¹
            logging.info("æ£€æµ‹åˆ°å®Œæ•´æ£€æŸ¥ç‚¹æ ¼å¼")
            
            # åŠ è½½æ¨¡å‹æƒé‡
            model.load_state_dict(checkpoint['model_state_dict'])
            
            # å°è¯•åŠ è½½optimizerçŠ¶æ€
            if 'optimizer_state_dict' in checkpoint and optimizer is not None:
                try:
                    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
                    logging.info("æˆåŠŸæ¢å¤optimizerçŠ¶æ€")
                except Exception as e:
                    logging.warning(f"æ— æ³•æ¢å¤optimizerçŠ¶æ€: {e}")
            
            # å°è¯•åŠ è½½schedulerçŠ¶æ€
            if 'scheduler_state_dict' in checkpoint and scheduler is not None:
                try:
                    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
                    logging.info("æˆåŠŸæ¢å¤schedulerçŠ¶æ€")
                except Exception as e:
                    logging.warning(f"æ— æ³•æ¢å¤schedulerçŠ¶æ€: {e}")
            
            return {
                'epoch': checkpoint.get('epoch', 0),
                'best_val_loss': checkpoint.get('best_val_loss', float('inf')),
                'best_ndcg': checkpoint.get('best_ndcg', 0.0),
                'val_loss': checkpoint.get('val_loss', float('inf')),
                'val_ppl': checkpoint.get('val_ppl', float('inf')),
                'hr': checkpoint.get('hr', 0.0),
                'ndcg': checkpoint.get('ndcg', 0.0),
                'patience_counter': checkpoint.get('patience_counter', 0)
            }
            
        else:
            # ä»…æƒé‡çš„æ£€æŸ¥ç‚¹
            logging.info("æ£€æµ‹åˆ°æƒé‡æ ¼å¼ï¼Œä»…æ¢å¤æ¨¡å‹æƒé‡")
            
            # å¤„ç†å¯èƒ½çš„é”®åä¸åŒ¹é…
            if hasattr(checkpoint, 'keys'):
                # å¦‚æœæ˜¯state_dictæ ¼å¼
                model.load_state_dict(checkpoint, strict=False)
            else:
                # å¦‚æœæ˜¯ç›´æ¥çš„æƒé‡
                logging.warning("æ— æ³•è¯†åˆ«çš„æ£€æŸ¥ç‚¹æ ¼å¼ï¼Œè·³è¿‡åŠ è½½")
                return None
            
            return {
                'epoch': 0,
                'best_val_loss': float('inf'),
                'best_ndcg': 0.0,
                'val_loss': float('inf'),
                'val_ppl': float('inf'),
                'hr': 0.0,
                'ndcg': 0.0,
                'patience_counter': 0
            }
            
    except Exception as e:
        logging.error(f"åŠ è½½æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
        return None

def save_checkpoint(checkpoint_path, model, optimizer, scheduler, epoch, metrics_dict, config, num_items):
    """
    æ£€æŸ¥ç‚¹ä¿å­˜å‡½æ•°ï¼Œä¿å­˜å®Œæ•´çš„è®­ç»ƒçŠ¶æ€
    """
    checkpoint_data = {
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict() if optimizer else None,
        'scheduler_state_dict': scheduler.state_dict() if scheduler else None,
        'config': config,
        'num_items': num_items,
        **metrics_dict  # å±•å¼€æ‰€æœ‰æŒ‡æ ‡
    }
    
    torch.save(checkpoint_data, checkpoint_path)

def main():
    # 1. å‚æ•°è§£æå’Œé…ç½®åŠ è½½
    parser = argparse.ArgumentParser(description="Train GENIUS-Rec Model")
    parser.add_argument('--encoder_weights_path', type=str, default=None, help='Path to pre-trained HSTU encoder weights.')
    parser.add_argument('--freeze_encoder', action='store_true', help='Freeze encoder weights.')
    parser.add_argument('--resume_from', type=str, default=None, help='Path to checkpoint to resume from.')
    parser.add_argument('--save_dir', type=str, default=None, help='Directory to save checkpoints.')
    args = parser.parse_args()

    config = get_config()
    
    # 2. ç¯å¢ƒè®¾ç½®
    device = torch.device(config['device'])
    random.seed(config['seed'])
    np.random.seed(config['seed'])
    torch.manual_seed(config['seed'])
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(config['seed'])

    # 3. æ—¥å¿—è®¾ç½®
    log_dir_path = pathlib.Path(config['data']['log_dir'])
    log_dir_path.mkdir(parents=True, exist_ok=True)
    log_file = log_dir_path / 'train_genius_rec.log'
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file),
            logging.StreamHandler()
        ]
    )
    
    logging.info("=== Starting GENIUS-Rec Training ===")
    logging.info(f"Device: {device}")
    logging.info(f"Arguments: {args}")

    # 4. æ•°æ®åŠ è½½
    logging.info("Loading data from processed directory...")
    with open(config['data']['id_maps_file'], 'rb') as f:
        id_maps = pickle.load(f)

    train_dataset = Seq2SeqRecDataset(config['data']['train_file'], config['decoder_model']['max_seq_len'])
    val_dataset = Seq2SeqRecDataset(config['data']['validation_file'], config['decoder_model']['max_seq_len'])
    eval_dataset = ValidationDataset(
        config['data']['validation_file'], 
        config['encoder_model']['max_len']
    )
    
    train_loader = DataLoader(train_dataset, batch_size=config['finetune']['batch_size'], shuffle=True, num_workers=config['finetune']['num_workers'])
    val_loader = DataLoader(val_dataset, batch_size=config['finetune']['batch_size'], shuffle=False, num_workers=config['finetune']['num_workers'])
    eval_loader = DataLoader(eval_dataset, batch_size=config['finetune']['batch_size'], shuffle=False, num_workers=config['finetune']['num_workers'])
    
    num_items = id_maps['num_items'] + 1
    pad_token_id = config['pad_token_id']
    top_k = config['evaluation']['top_k']
    
    logging.info(f"ğŸ“Š Dataset Info:")
    logging.info(f"  - Training samples: {len(train_dataset)}")
    logging.info(f"  - Validation samples: {len(val_dataset)}")
    logging.info(f"  - Evaluation samples: {len(eval_dataset)}")
    logging.info(f"  - Total items: {num_items}")

    # 5. æ–‡æœ¬åµŒå…¥åŠ è½½
    logging.info("Loading pre-computed and filtered text embeddings...")
    text_embedding_file = config['data']['data_dir'] / 'book_gemini_embeddings_filtered.npy'
    try:
        text_embeddings_dict = np.load(text_embedding_file, allow_pickle=True).item()
    except FileNotFoundError:
        logging.error(f"Filtered embedding file not found at '{text_embedding_file}'! Please run filter_embeddings.py first.")
        return

    text_embedding_dim = next(iter(text_embeddings_dict.values())).shape[0]
    text_embedding_matrix = torch.zeros(num_items, text_embedding_dim, dtype=torch.float)
    
    item_asin_map = id_maps['item_map']
    loaded_count = 0
    for asin, embedding in text_embeddings_dict.items():
        if asin in item_asin_map:
            item_id = item_asin_map[asin]
            text_embedding_matrix[item_id] = torch.tensor(embedding, dtype=torch.float)
            loaded_count += 1
    
    logging.info(f"Successfully loaded and mapped {loaded_count} text embeddings.")
    if loaded_count == 0:
        logging.warning("No embeddings were mapped. The content expert will not function.")

    # 6. æ¨¡å‹åˆå§‹åŒ–
    config['encoder_model']['item_num'] = num_items
    config['decoder_model']['num_items'] = num_items
    config['decoder_model']['text_embedding_dim'] = text_embedding_dim
    model = GENIUSRecModel(config['encoder_model'], config['decoder_model']).to(device)
    logging.info("GENIUS-Rec model created.")

    # 7. é¢„è®­ç»ƒæƒé‡åŠ è½½ï¼ˆå¦‚æœæŒ‡å®šï¼‰
    if args.encoder_weights_path:
        try:
            logging.info(f"Loading encoder weights from: {args.encoder_weights_path}")
            
            # è·¨å¹³å°è·¯å¾„å¤„ç†
            if platform.system() == "Linux" and os.path.exists(args.encoder_weights_path):
                temp_windows_path = getattr(pathlib, 'WindowsPath', None)
                pathlib.WindowsPath = pathlib.PosixPath
            
            checkpoint = torch.load(args.encoder_weights_path, map_location=device, weights_only=False)
            
            if platform.system() == "Linux" and 'temp_windows_path' in locals():
                pathlib.WindowsPath = temp_windows_path
            
            # å¤„ç†ä¸åŒçš„checkpointæ ¼å¼
            if 'model_state_dict' in checkpoint:
                encoder_state_dict = checkpoint['model_state_dict']
                logging.info("Found 'model_state_dict' in checkpoint")
            else:
                encoder_state_dict = checkpoint
                logging.info("Using checkpoint directly as state_dict")
            
            # å¤„ç†item_numä¸åŒ¹é…é—®é¢˜
            current_item_embedding_size = model.encoder.item_embedding.weight.shape
            checkpoint_item_embedding_size = encoder_state_dict.get('item_embedding.weight', torch.empty(0)).shape
            
            if checkpoint_item_embedding_size != current_item_embedding_size:
                logging.warning(f"Item embedding size mismatch:")
                logging.warning(f"   Current model: {current_item_embedding_size}")
                logging.warning(f"   Checkpoint: {checkpoint_item_embedding_size}")
                logging.info("   Adjusting item embedding size...")
                
                if len(checkpoint_item_embedding_size) > 0:
                    old_embedding = encoder_state_dict['item_embedding.weight']
                    new_embedding = model.encoder.item_embedding.weight.data.clone()
                    min_items = min(old_embedding.shape[0], new_embedding.shape[0])
                    new_embedding[:min_items] = old_embedding[:min_items]
                    encoder_state_dict['item_embedding.weight'] = new_embedding
                    logging.info(f"   âœ… Copied {min_items} item embeddings")
            
            missing_keys, unexpected_keys = model.encoder.load_state_dict(encoder_state_dict, strict=False)
            
            if missing_keys:
                logging.warning(f"Missing keys: {missing_keys}")
            if unexpected_keys:
                logging.warning(f"Unexpected keys: {unexpected_keys}")
                
            logging.info("âœ… Pre-trained HSTU encoder weights loaded successfully")
            
        except Exception as e:
            logging.error(f"Could not load encoder weights: {e}")
            logging.info("Training from scratch...")

    if args.freeze_encoder:
        for param in model.encoder.parameters():
            param.requires_grad = False
        logging.info("ğŸ”’ Encoder weights frozen.")

    # 8. æ–‡æœ¬åµŒå…¥åŠ è½½åˆ°æ¨¡å‹
    model.decoder.load_text_embeddings(text_embedding_matrix.to(device))

    # 9. ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
    optimizer = torch.optim.AdamW(
        filter(lambda p: p.requires_grad, model.parameters()), 
        lr=config['finetune']['learning_rate']['decoder_lr'],
        weight_decay=config['finetune'].get('weight_decay', 0.01)
    )
    criterion = nn.CrossEntropyLoss(ignore_index=pad_token_id, label_smoothing=0.1)

    # 10. å­¦ä¹ ç‡è°ƒåº¦å™¨
    num_training_steps = len(train_loader) * config['finetune']['num_epochs']
    num_warmup_steps = config['finetune'].get('warmup_steps', 500)
    scheduler = get_linear_schedule_with_warmup(
        optimizer, 
        num_warmup_steps=num_warmup_steps, 
        num_training_steps=num_training_steps
    )
    
    logging.info(f"Using learning rate scheduler with {num_warmup_steps} warmup steps and {num_training_steps} total training steps.")

    # 11. æ£€æŸ¥ç‚¹ç›®å½•è®¾ç½®
    if args.save_dir:
        output_dir = pathlib.Path(args.save_dir)
    else:
        output_dir = config['data']['checkpoint_dir']
    output_dir.mkdir(parents=True, exist_ok=True)
    
    best_model_path = output_dir / 'genius_rec_moe_best.pth'
    best_ranking_model_path = output_dir / 'genius_rec_moe_best_ranking.pth'
    latest_model_path = output_dir / 'genius_rec_moe_latest.pth'

    # 12. æ–­ç‚¹ç»­ä¼ 
    start_epoch = 0
    best_val_loss = float('inf')
    best_ndcg = 0.0
    patience_counter = 0
    
    resume_path = args.resume_from or latest_model_path
    if os.path.exists(resume_path):
        resume_info = load_checkpoint(resume_path, model, optimizer, scheduler, device)
        if resume_info:
            start_epoch = resume_info['epoch'] + 1
            best_val_loss = resume_info['best_val_loss']
            best_ndcg = resume_info['best_ndcg']
            patience_counter = resume_info['patience_counter']
            logging.info(f"âœ… æˆåŠŸæ¢å¤è®­ç»ƒçŠ¶æ€! ä» Epoch {start_epoch} ç»§ç»­")
            logging.info(f"   - Best Val Loss: {best_val_loss:.4f}")
            logging.info(f"   - Best NDCG: {best_ndcg:.4f}")

    # 13. è®­ç»ƒä¸»å¾ªç¯
    logging.info("=== Starting Training Loop ===")
    
    for epoch in range(start_epoch, config['finetune']['num_epochs']):
        # è®­ç»ƒä¸€ä¸ªepoch
        avg_train_loss = train_one_epoch(
            model, train_loader, criterion, optimizer, scheduler, 
            device, epoch, config['finetune']['num_epochs'], pad_token_id
        )
        
        # æ¯ä¸ªepochéƒ½è¿›è¡Œå®Œæ•´è¯„ä¼°
        eval_results = evaluate_model(
            model, val_loader, eval_loader, criterion, device, 
            epoch, config['finetune']['num_epochs'], pad_token_id, 
            num_items, top_k
        )
        
        # æ—¥å¿—è¾“å‡º
        logging.info(f"Epoch {epoch+1}/{config['finetune']['num_epochs']} Results:")
        logging.info(f"  ğŸ“ˆ Train Loss: {avg_train_loss:.4f}")
        logging.info(f"  ğŸ“‰ Val Loss: {eval_results['val_loss']:.4f}")
        logging.info(f"  ğŸ“Š Val PPL: {eval_results['val_ppl']:.4f}")
        logging.info(f"  âš–ï¸  Behavior Weight: {eval_results['avg_behavior_weight']:.4f}")
        logging.info(f"  âš–ï¸  Content Weight: {eval_results['avg_content_weight']:.4f}")
        logging.info(f"  ğŸ¯ HR@{top_k}: {eval_results['hr']:.4f}")
        logging.info(f"  ğŸ¯ NDCG@{top_k}: {eval_results['ndcg']:.4f}")
        logging.info(f"  ğŸ“Š Evaluated samples: {eval_results['evaluated_samples']}")

        # å‡†å¤‡ä¿å­˜çš„æŒ‡æ ‡
        save_metrics = {
            'best_val_loss': best_val_loss,
            'best_ndcg': best_ndcg,
            'patience_counter': patience_counter,
            **eval_results
        }

        # ä¿å­˜æœ€æ–°æ£€æŸ¥ç‚¹
        save_checkpoint(
            latest_model_path, model, optimizer, scheduler, 
            epoch, save_metrics, config, num_items
        )
        logging.info(f"ä¿å­˜æœ€æ–°æ£€æŸ¥ç‚¹åˆ°: {latest_model_path}")

        # åŸºäºéªŒè¯lossä¿å­˜æœ€ä½³æ¨¡å‹
        if eval_results['val_loss'] < best_val_loss:
            best_val_loss = eval_results['val_loss']
            patience_counter = 0
            save_metrics['best_val_loss'] = best_val_loss
            
            save_checkpoint(
                best_model_path, model, optimizer, scheduler, 
                epoch, save_metrics, config, num_items
            )
            logging.info(f"ğŸ‰ å‘ç°æ–°çš„æœ€ä½³æ¨¡å‹ (by loss)! Val Loss: {best_val_loss:.4f}")
        else:
            patience_counter += 1
            
        # åŸºäºNDCGä¿å­˜æœ€ä½³æ¨¡å‹
        if eval_results['ndcg'] > best_ndcg:
            best_ndcg = eval_results['ndcg']
            save_metrics['best_ndcg'] = best_ndcg
            
            save_checkpoint(
                best_ranking_model_path, model, optimizer, scheduler, 
                epoch, save_metrics, config, num_items
            )
            logging.info(f"ğŸŒŸ å‘ç°æ–°çš„æœ€ä½³æ¨¡å‹ (by NDCG)! NDCG: {best_ndcg:.4f}")

        # æ—©åœæ£€æŸ¥
        early_stopping_patience = config['finetune'].get('early_stopping_patience', 10)
        if patience_counter >= early_stopping_patience:
            logging.info(f"è§¦å‘æ—©åœ! è¿ç»­ {patience_counter} ä¸ªepochæ€§èƒ½æœªæå‡")
            break
            
        logging.info(f"è€å¿ƒè®¡æ•°: {patience_counter}/{early_stopping_patience}")
        logging.info("-" * 80)

    # 14. è®­ç»ƒå®Œæˆæ€»ç»“
    completed_epochs = epoch + 1 if 'epoch' in locals() else start_epoch
    logging.info("=== Training Finished ===")
    logging.info(f"è®­ç»ƒæ€»è½®æ¬¡: {completed_epochs}/{config['finetune']['num_epochs']}")
    logging.info(f"ğŸ“ˆ Final Results:")
    logging.info(f"  - Best validation loss: {best_val_loss:.4f}")
    logging.info(f"  - Best NDCG@{top_k}: {best_ndcg:.4f}")
    logging.info(f"æ£€æŸ¥ç‚¹ä¿å­˜ä½ç½®:")
    logging.info(f"  - Latest: {latest_model_path}")
    logging.info(f"  - Best (Loss): {best_model_path}")
    logging.info(f"  - Best (Ranking): {best_ranking_model_path}")

if __name__ == '__main__':
    main()