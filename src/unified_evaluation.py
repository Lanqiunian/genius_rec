"""
ç»Ÿä¸€è¯„ä¼°æ¨¡å— (unified_evaluation.py)

è¯¥æ¨¡å—æ•´åˆäº†æ‰€æœ‰æ¨¡å‹(HSTU, Baseline, GENIUS-Rec)çš„è¯„ä¼°å‡½æ•°ï¼Œ
ç¡®ä¿ä¸‰ä¸ªæ¨¡å‹ä½¿ç”¨å®Œå…¨ä¸€è‡´çš„è¯„ä¼°é€»è¾‘å’ŒæŒ‡æ ‡è®¡ç®—æ–¹æ³•ã€‚
"""

import math
import torch
import numpy as np
import random
import torch.nn.functional as F
from tqdm import tqdm
from torch.utils.data import Dataset, DataLoader
import pandas as pd

from src.metrics import compute_hr_ndcg_full, compute_hr_ndcg_batch


class ValidationDataset(Dataset):
    """
    ç”¨äºæ’åºæŒ‡æ ‡è¯„ä¼°çš„æ•°æ®é›†ï¼š
    - åªä»éªŒè¯/æµ‹è¯•é›†ä¸­å–æ•°æ®
    - ä½¿ç”¨Leave-One-Outæ–¹å¼è¯„ä¼°
    - ç¡®ä¿è®­ç»ƒæ—¶æ²¡æœ‰è§è¿‡å®Œæ•´åºåˆ—
    """
    def __init__(self, data_path, max_len, pad_token_id=0):
        self.data = pd.read_parquet(data_path)
        self.max_len = max_len
        self.pad_token_id = pad_token_id

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        full_seq = self.data.iloc[idx]['history']
        # Leave-One-Out: æœ€åä¸€ä¸ªä½œä¸ºç›®æ ‡ï¼Œå…¶ä½™ä½œä¸ºè¾“å…¥
        ground_truth_item = full_seq[-1]
        input_seq = full_seq[:-1]
        
        # æˆªæ–­å’Œå¡«å……
        if len(input_seq) > self.max_len:
            input_seq = input_seq[-self.max_len:]
        
        padded_input_seq = np.full(self.max_len, self.pad_token_id, dtype=np.int64)
        padded_input_seq[-len(input_seq):] = input_seq
        
        return {
            'input_ids': torch.tensor(padded_input_seq, dtype=torch.long),
            'ground_truth': torch.tensor(ground_truth_item, dtype=torch.long)
        }


def evaluate_model_validation(model, val_loader, criterion, device, pad_token_id):
    """
    éªŒè¯é›†è¯„ä¼°ï¼šè®¡ç®—losså’Œperplexityï¼ˆä¸è®¡ç®—æ’åºæŒ‡æ ‡ï¼‰
    é€‚ç”¨äºåŸºç¡€è¯„ä¼°éœ€æ±‚
    """
    model.eval()
    total_loss = 0
    total_tokens = 0
    
    with torch.no_grad():
        for batch in val_loader:
            source_ids = batch['source_ids'].to(device)
            decoder_input_ids = batch['decoder_input_ids'].to(device)
            labels = batch['labels'].to(device)
            source_padding_mask = (source_ids == pad_token_id)
            
            # å‰å‘ä¼ æ’­
            logits = model(source_ids, decoder_input_ids, source_padding_mask)
            
            # è®¡ç®—æŸå¤±
            loss = criterion(logits.view(-1, logits.size(-1)), labels.view(-1))
            
            # ç»Ÿè®¡épaddingçš„tokenæ•°é‡
            non_padding_mask = (labels != pad_token_id).float()
            num_tokens = non_padding_mask.sum().item()
            
            total_loss += loss.item() * num_tokens
            total_tokens += num_tokens
    
    # è®¡ç®—å¹³å‡æŸå¤±å’Œå›°æƒ‘åº¦
    avg_loss = total_loss / total_tokens if total_tokens > 0 else 0
    perplexity = math.exp(avg_loss) if avg_loss < 20 else float('inf')
    
    return {
        'val_loss': avg_loss,
        'val_ppl': perplexity
    }


def evaluate_model_validation_with_ranking(model, val_loader, criterion, device, epoch, num_epochs, pad_token_id, num_candidates=None, top_k=10):
    """
    éªŒè¯é›†è¯„ä¼°ï¼šè®¡ç®—lossã€pplå’Œæ’åºæŒ‡æ ‡
    
    å®Œå…¨ä½¿ç”¨ä¸HSTU/baselineä¸€è‡´çš„è¯„ä¼°é€»è¾‘ï¼Œç¡®ä¿æŒ‡æ ‡çš„å¯æ¯”æ€§
    
    Args:
        model: å¾…è¯„ä¼°çš„æ¨¡å‹
        val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
        criterion: æŸå¤±å‡½æ•°
        device: è®¾å¤‡
        epoch: å½“å‰è½®æ¬¡
        num_epochs: æ€»è½®æ¬¡
        pad_token_id: å¡«å……æ ‡è®°ID
        num_candidates: è¯„ä¼°æ–¹å¼æ§åˆ¶å‚æ•°
                        - None: ä½¿ç”¨å…¨é‡è¯„ä¼°ï¼ˆä¸æ‰€æœ‰ç‰©å“è®¡ç®—ç›¸ä¼¼åº¦ï¼‰- ä¸HSTUå’Œbaselineå®Œå…¨ä¸€è‡´
                        - æ•´æ•°å€¼(å¦‚500): ä½¿ç”¨é‡‡æ ·è¯„ä¼°ï¼ˆæ¯ä¸ªç”¨æˆ·éšæœºæŠ½å–n-1ä¸ªè´Ÿæ ·æœ¬+1ä¸ªæ­£æ ·æœ¬ï¼‰- é€Ÿåº¦æ›´å¿«
        top_k: æ¨èåˆ—è¡¨é•¿åº¦K
    """
    model.eval()
    
    total_loss_tokens = 0.0
    total_tokens = 0
    total_gate_weights = None
    total_valid_batches = 0
    
    # æ’åºæŒ‡æ ‡è®¡ç®—
    hr_total = 0.0
    ndcg_total = 0.0
    total_samples = 0
    
    # é¢„å…ˆè®¡ç®—æ‰€æœ‰ç‰©å“åµŒå…¥
    with torch.no_grad():
        item_num = model.encoder.item_embedding.num_embeddings
        all_item_ids = torch.arange(1, item_num, device=device)
        all_item_embeddings = model.encoder.item_embedding(all_item_ids)  # [num_items-1, embed_dim]
    
    # æ ¹æ®è¯„ä¼°æ¨¡å¼è®¾ç½®ä¸åŒçš„è¿›åº¦æ¡æè¿°
    if num_candidates is not None and num_candidates > 0:
        progress_bar = tqdm(val_loader, desc=f"Epoch {epoch+1}/{num_epochs} [Validation - Sampled Eval ({num_candidates} candidates)]")
    else:
        progress_bar = tqdm(val_loader, desc=f"Epoch {epoch+1}/{num_epochs} [Validation - Full Eval]")

    with torch.no_grad():
        for batch in progress_bar:
            source_ids = batch['source_ids'].to(device)
            decoder_input_ids = batch['decoder_input_ids'].to(device)
            labels = batch['labels'].to(device)
            source_padding_mask = (source_ids == pad_token_id)

            # è®¡ç®—æ¨¡å‹è¾“å‡ºï¼ˆåŒ…æ‹¬æŸå¤±è®¡ç®—æ‰€éœ€çš„logitsï¼‰
            logits, gate_weights = model(source_ids, decoder_input_ids, source_padding_mask, return_weights=True)
            
            # è®¡ç®—æŸå¤±
            loss = criterion(logits.view(-1, logits.size(-1)), labels.view(-1))
            
            # ç»Ÿè®¡épaddingçš„tokenæ•°é‡
            non_padding_mask = (labels != pad_token_id).float()
            num_tokens = non_padding_mask.sum().item()
            
            total_loss_tokens += loss.item() * num_tokens
            total_tokens += num_tokens
            
            # è®°å½•ä¸“å®¶æƒé‡
            if gate_weights is not None:
                # ğŸ”§ ä¿®å¤ï¼šä»åŸç†ä¸Šç†è§£å’Œå¤„ç†é—¨æ§æƒé‡
                # gate_weightså½¢çŠ¶ä¸º [batch_size, target_len, num_experts]
                # labelså½¢çŠ¶ä¸º [batch_size, target_len]
                
                # 1. åˆ›å»ºä¸€ä¸ªæ©ç ï¼Œæ ‡è¯†å“ªäº›ä½ç½®æ˜¯æœ‰æ•ˆæ ‡ç­¾ï¼ˆépadï¼‰
                label_mask = (labels != pad_token_id).float().unsqueeze(-1)  # [B, T, 1]
                
                # 2. åº”ç”¨æ©ç ï¼Œåªè€ƒè™‘æœ‰æ•ˆä½ç½®çš„æƒé‡
                valid_gate_weights = gate_weights * label_mask
                
                # 3. å¯¹æ¯ä¸ªbatchçš„æœ‰æ•ˆä½ç½®å–å¹³å‡ï¼ˆé¿å…åˆ‡ç‰‡æ“ä½œï¼‰
                batch_sum = valid_gate_weights.sum(dim=1)  # [B, num_experts]
                batch_count = label_mask.sum(dim=1)  # [B, 1]
                # é˜²æ­¢é™¤é›¶
                batch_mean = batch_sum / (batch_count + 1e-8)  # [B, num_experts]
                
                # 4. å†å¯¹æ•´ä¸ªbatchå–å¹³å‡å¾—åˆ°æœ€ç»ˆæƒé‡
                masked_gate_weights = batch_mean.mean(dim=0)  # [num_experts]
                
                if total_gate_weights is None:
                    total_gate_weights = masked_gate_weights
                    total_valid_batches = 1
                else:
                    total_gate_weights += masked_gate_weights
                    total_valid_batches += 1
            
            # å…¨é‡è¯„ä¼°æ’åºæŒ‡æ ‡è®¡ç®—
            # å¯¹æ•´ä¸ªæ‰¹æ¬¡è·å–ç¼–ç å™¨è¾“å‡º
            encoder_outputs = model.encoder(source_ids)  # [B, L, D]
            user_embeddings = encoder_outputs[:, -1, :]  # [B, D] - å–æ¯ä¸ªåºåˆ—çš„æœ€åä¸€ä¸ªä½ç½®ä½œä¸ºç”¨æˆ·è¡¨ç¤º
            
            # ä»æ ‡ç­¾ä¸­æå–çœŸå®ç›®æ ‡ç‰©å“ID
            target_item_ids = []
            for i in range(source_ids.size(0)):
                valid_positions = (labels[i] != pad_token_id).nonzero(as_tuple=True)[0]
                if len(valid_positions) == 0:
                    # å¦‚æœæ²¡æœ‰æœ‰æ•ˆæ ‡ç­¾ï¼Œç”¨0å¡«å……ï¼ˆåé¢ä¼šè·³è¿‡ï¼‰
                    target_item_ids.append(0)
                else:
                    # å–ç¬¬ä¸€ä¸ªépaddingçš„æ ‡ç­¾ä½œä¸ºç›®æ ‡
                    target_item_ids.append(labels[i, valid_positions[0]].item())
            
            target_item_ids = torch.tensor(target_item_ids, device=device)
            
            # ä½¿ç”¨ç»Ÿä¸€çš„è¯„ä¼°æŒ‡æ ‡è®¡ç®—
            if num_candidates is not None and num_candidates > 0:
                # ä½¿ç”¨é‡‡æ ·è¯„ä¼°ï¼ˆéšæœºæŠ½å–nä¸ªè´Ÿæ ·æœ¬+1ä¸ªæ­£æ ·æœ¬ï¼‰
                batch_size = user_embeddings.size(0)
                
                # ä¸ºæ¯ä¸ªç”¨æˆ·é€‰æ‹©éšæœºè´Ÿæ ·æœ¬
                hr_batch_sum, ndcg_batch_sum = 0.0, 0.0
                valid_batch_samples = 0
                
                for i in range(batch_size):
                    target_id = target_item_ids[i].item()
                    if target_id == 0:
                        continue  # è·³è¿‡æ— æ•ˆæ ·æœ¬
                    
                    valid_batch_samples += 1
                    
                    # éšæœºé€‰æ‹©num_candidates-1ä¸ªè´Ÿæ ·æœ¬ID (æ’é™¤0å’Œç›®æ ‡ID)
                    candidate_ids = set(range(1, item_num))
                    candidate_ids.discard(target_id)  # æ’é™¤æ­£æ ·æœ¬
                    neg_ids = random.sample(list(candidate_ids), min(num_candidates-1, len(candidate_ids)))
                    
                    # åˆå¹¶æ­£è´Ÿæ ·æœ¬
                    all_candidate_ids = [target_id] + neg_ids
                    random.shuffle(all_candidate_ids)  # æ‰“ä¹±é¡ºåº
                    
                    # è½¬æ¢ä¸ºå¼ é‡
                    all_candidate_ids = torch.tensor(all_candidate_ids, device=device)
                    candidate_embeddings = model.encoder.item_embedding(all_candidate_ids)
                    
                    # è®¡ç®—å•ä¸ªç”¨æˆ·çš„æŒ‡æ ‡
                    # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬çŸ¥é“ç›®æ ‡IDå°±æ˜¯all_candidate_ids[0]
                    target_position = (all_candidate_ids == target_id).nonzero(as_tuple=True)[0].item()
                    target_id_tensor = torch.tensor([target_id], device=device)
                    
                    # ä½¿ç”¨å•ä¸ªç”¨æˆ·çš„è®¡ç®—å‡½æ•°
                    user_emb = user_embeddings[i].unsqueeze(0)  # [1, D]
                    hr, ndcg = compute_hr_ndcg_full(
                        user_emb,
                        F.normalize(candidate_embeddings, p=2, dim=1),
                        torch.tensor([target_id], device=device),
                        k=top_k
                    )
                    
                    hr_batch_sum += hr
                    ndcg_batch_sum += ndcg
                
                # è®¡ç®—æ‰¹æ¬¡å¹³å‡å€¼
                if valid_batch_samples > 0:
                    hr = hr_batch_sum / valid_batch_samples
                    ndcg = ndcg_batch_sum / valid_batch_samples
                else:
                    hr, ndcg = 0.0, 0.0
            else:
                # ä½¿ç”¨å…¨é‡è¯„ä¼°ï¼ˆä¸æ‰€æœ‰ç‰©å“è®¡ç®—ç›¸ä¼¼åº¦ï¼‰
                hr, ndcg = compute_hr_ndcg_full(
                    user_embeddings,
                    all_item_embeddings,
                    target_item_ids,
                    k=top_k
                )
            
            hr_total += hr * source_ids.size(0)
            ndcg_total += ndcg * source_ids.size(0)
            total_samples += source_ids.size(0)
            
            progress_bar.set_postfix(loss=loss.item())
        
        # æ¸…ç†GPUæ˜¾å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

    # è®¡ç®—æœ€ç»ˆæŒ‡æ ‡
    avg_loss = total_loss_tokens / total_tokens if total_tokens > 0 else 0
    perplexity = math.exp(avg_loss) if avg_loss < 20 else float('inf')
    avg_gate_weights = total_gate_weights / total_valid_batches if total_valid_batches > 0 else None
    
    # æ’åºæŒ‡æ ‡ - å®Œå…¨å¯¹é½HSTUè®¡ç®—æ–¹å¼
    avg_hr = hr_total / total_samples if total_samples > 0 else 0.0
    avg_ndcg = ndcg_total / total_samples if total_samples > 0 else 0.0
    
    result = {
        'val_loss': avg_loss,
        'val_ppl': perplexity,
        'val_hr': avg_hr,          # ä¸HSTUå®Œå…¨å¯¹é½çš„HR@K
        'val_ndcg': avg_ndcg,      # ä¸HSTUå®Œå…¨å¯¹é½çš„NDCG@K
        'avg_gate_weights': avg_gate_weights,
        'evaluated_samples': total_samples
    }
    
    # åŠ¨æ€æ·»åŠ ä¸“å®¶æƒé‡ä¿¡æ¯
    if avg_gate_weights is not None:
        enabled_experts = [k for k, v in model.decoder.expert_config["experts"].items() if v]
        for i, expert_name in enumerate(enabled_experts):
            if i < len(avg_gate_weights):
                result[f'avg_{expert_name}_weight'] = avg_gate_weights[i].item()
    
    return result


def evaluate_model_test(model, test_loader, device, item_num, num_candidates=None, top_k=10):
    """
    æµ‹è¯•é›†è¯„ä¼°ï¼šåªè®¡ç®—æ’åºæŒ‡æ ‡ï¼Œè®­ç»ƒç»“æŸåè°ƒç”¨ä¸€æ¬¡
    
    ä½¿ç”¨ä¸HSTUå®Œå…¨ç›¸åŒçš„è¯„ä¼°é€»è¾‘
    
    Args:
        model: å¾…è¯„ä¼°çš„æ¨¡å‹
        test_loader: æµ‹è¯•æ•°æ®åŠ è½½å™¨
        device: è®¾å¤‡
        item_num: ç‰©å“æ€»æ•°
        num_candidates: è¯„ä¼°æ–¹å¼æ§åˆ¶å‚æ•°
                        - None: ä½¿ç”¨å…¨é‡è¯„ä¼°ï¼ˆä¸æ‰€æœ‰ç‰©å“è®¡ç®—ç›¸ä¼¼åº¦ï¼‰- ä¸HSTUå®Œå…¨ä¸€è‡´
                        - æ•´æ•°å€¼: ä½¿ç”¨é‡‡æ ·è¯„ä¼°ï¼ˆæ¯ä¸ªç”¨æˆ·éšæœºæŠ½å–n-1ä¸ªè´Ÿæ ·æœ¬+1ä¸ªæ­£æ ·æœ¬ï¼‰
        top_k: æ¨èåˆ—è¡¨é•¿åº¦K
    """
    model.eval()
    hr_total = 0.0
    ndcg_total = 0.0
    total_samples = 0
    
    with torch.no_grad():
        # é¢„å…ˆè®¡ç®—æ‰€æœ‰ç‰©å“åµŒå…¥
        all_item_ids = torch.arange(1, item_num, device=device)
        all_item_embeddings = model.encoder.item_embedding(all_item_ids)
        
        # æ ¹æ®è¯„ä¼°æ¨¡å¼è®¾ç½®ä¸åŒçš„è¿›åº¦æ¡æè¿°
        if num_candidates is not None and num_candidates > 0:
            progress_bar = tqdm(test_loader, desc=f"Test Set Evaluation - Sampled Eval ({num_candidates} candidates)")
        else:
            progress_bar = tqdm(test_loader, desc="Test Set Evaluation - Full Eval")
        
        for batch in progress_bar:
            input_ids = batch['input_ids'].to(device)
            ground_truth_ids = batch['ground_truth'].to(device)

            # è·å–ç”¨æˆ·åµŒå…¥
            encoder_output = model.encoder(input_ids)
            user_embeddings = encoder_output[:, -1, :]  # å–æœ€åä¸€ä¸ªä½ç½®
            
            # ä½¿ç”¨ç»Ÿä¸€çš„è¯„ä¼°å‡½æ•°
            batch_size = input_ids.size(0)
            
            if num_candidates is not None and num_candidates > 0:
                # ä½¿ç”¨é‡‡æ ·è¯„ä¼°ï¼ˆéšæœºæŠ½å–nä¸ªè´Ÿæ ·æœ¬+1ä¸ªæ­£æ ·æœ¬ï¼‰
                hr_batch_sum, ndcg_batch_sum = 0.0, 0.0
                valid_batch_samples = 0
                
                for i in range(batch_size):
                    target_id = ground_truth_ids[i].item()
                    if target_id == 0:
                        continue  # è·³è¿‡æ— æ•ˆæ ·æœ¬
                    
                    valid_batch_samples += 1
                    
                    # éšæœºé€‰æ‹©num_candidates-1ä¸ªè´Ÿæ ·æœ¬ID (æ’é™¤0å’Œç›®æ ‡ID)
                    candidate_ids = set(range(1, item_num))
                    candidate_ids.discard(target_id)  # æ’é™¤æ­£æ ·æœ¬
                    neg_ids = random.sample(list(candidate_ids), min(num_candidates-1, len(candidate_ids)))
                    
                    # åˆå¹¶æ­£è´Ÿæ ·æœ¬
                    all_candidate_ids = [target_id] + neg_ids
                    random.shuffle(all_candidate_ids)  # æ‰“ä¹±é¡ºåº
                    
                    # è½¬æ¢ä¸ºå¼ é‡
                    all_candidate_ids = torch.tensor(all_candidate_ids, device=device)
                    candidate_embeddings = model.encoder.item_embedding(all_candidate_ids)
                    
                    # è®¡ç®—å•ä¸ªç”¨æˆ·çš„æŒ‡æ ‡
                    user_emb = user_embeddings[i].unsqueeze(0)  # [1, D]
                    hr, ndcg = compute_hr_ndcg_full(
                        user_emb,
                        F.normalize(candidate_embeddings, p=2, dim=1),
                        torch.tensor([target_id], device=device),
                        k=top_k
                    )
                    
                    hr_batch_sum += hr
                    ndcg_batch_sum += ndcg
                
                # è®¡ç®—æ‰¹æ¬¡å¹³å‡å€¼
                if valid_batch_samples > 0:
                    hr = hr_batch_sum / valid_batch_samples
                    ndcg = ndcg_batch_sum / valid_batch_samples
                else:
                    hr, ndcg = 0.0, 0.0
            else:
                # ä½¿ç”¨å…¨é‡è¯„ä¼°ï¼ˆä¸æ‰€æœ‰ç‰©å“è®¡ç®—ç›¸ä¼¼åº¦ï¼‰
                hr, ndcg = compute_hr_ndcg_full(
                    user_embeddings,
                    all_item_embeddings,
                    ground_truth_ids,
                    k=top_k
                )
            
            hr_total += hr * batch_size
            ndcg_total += ndcg * batch_size
            total_samples += batch_size
    
    avg_hr = hr_total / total_samples if total_samples > 0 else 0.0
    avg_ndcg = ndcg_total / total_samples if total_samples > 0 else 0.0
    
    return {
        'test_hr': avg_hr,
        'test_ndcg': avg_ndcg,
        'evaluated_samples': total_samples
    }


def evaluate_encoder(encoder, val_loader, device, top_k=10):
    """
    ç¼–ç å™¨è¯„ä¼°å‡½æ•°ï¼šç”¨äºHSTUç¼–ç å™¨çš„ç‹¬ç«‹è¯„ä¼°
    """
    encoder.eval()
    total_hr = 0.0
    total_ndcg = 0.0
    total_samples = 0
    
    with torch.no_grad():
        # è·å–æ‰€æœ‰ç‰©å“çš„åµŒå…¥
        item_num = encoder.item_embedding.num_embeddings
        all_item_ids = torch.arange(1, item_num, device=device)
        all_item_embeddings = encoder.item_embedding(all_item_ids)
        
        for batch in tqdm(val_loader, desc="Evaluating Encoder"):
            seq = batch[0].to(device)
            target_item_ids = batch[1].to(device).squeeze(1)
            
            # è·å–ç”¨æˆ·è¡¨ç¤ºï¼ˆåºåˆ—çš„æœ€åä¸€ä¸ªä½ç½®ï¼‰
            sequence_output = encoder.forward(seq)
            user_embeddings = sequence_output[:, -1, :]
            
            # ä½¿ç”¨ç»Ÿä¸€è¯„ä¼°å‡½æ•°
            batch_size = seq.size(0)
            hr, ndcg = compute_hr_ndcg_full(
                user_embeddings,
                all_item_embeddings,
                target_item_ids,
                k=top_k
            )
            
            total_hr += hr * batch_size
            total_ndcg += ndcg * batch_size
            total_samples += batch_size
    
    avg_hr = total_hr / total_samples if total_samples > 0 else 0.0
    avg_ndcg = total_ndcg / total_samples if total_samples > 0 else 0.0
    
    return avg_hr, avg_ndcg
