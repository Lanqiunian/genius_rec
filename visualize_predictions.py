import torch
import argparse
import pickle
from torch.utils.data import DataLoader

# å¯¼å…¥æ‚¨é¡¹ç›®ä¸­çš„æ ¸å¿ƒæ¨¡å—
from src.config import get_config
from src.dataset import Seq2SeqRecDataset
from src.GeniusRec import GENIUSRecModel
# 1. æœ€å¤§é•¿åº¦

# å¾ªç¯æ¬¡æ•°è¾¾åˆ°ä¸Šé™
# ä¿è¯ç¨‹åºä¸€å®šä¼šåœï¼Œæ˜¯æœ€åçš„åº•çº¿ã€‚

# 2. é‡å¤åœæ­¢

# åŒä¸€ä¸ªç‰©å“IDè¿ç»­å‡ºç°Næ¬¡
# é˜²æ­¢é™·å…¥æ— æ„ä¹‰çš„å¾ªç¯ï¼Œæé«˜ç”Ÿæˆåºåˆ—çš„è´¨é‡ã€‚è¿™æ˜¯å½“å‰æœ€ä¸»è¦çš„å¯å‘å¼åœæ­¢ç­–ç•¥ã€‚

# 3. å¡«å……ç¬¦åœæ­¢

# æ¨¡å‹ä¸»åŠ¨ç”Ÿæˆpad_token_id
# ç†æƒ³ä¸­çš„æ¨¡å‹â€œè‡ªç„¶â€åœæ­¢æ–¹å¼ï¼Œä½†åœ¨å½“å‰è®­ç»ƒè®¾ç½®ä¸‹åŸºæœ¬ä¸èµ·ä½œç”¨ã€‚

def generate_sequence(model, source_ids, max_len, pad_token_id, device):
    """
    ã€å·²ä¼˜åŒ–ã€‘ä½¿ç”¨è´ªå¿ƒç­–ç•¥è‡ªå›å½’åœ°ç”Ÿæˆåºåˆ—ã€‚
    æ–°å¢äº†é‡å¤æ£€æµ‹æœºåˆ¶æ¥æå‰åœæ­¢ã€‚
    """
    model.eval()
    
    encoder_output = model.encoder(source_ids)
    source_padding_mask = (source_ids == pad_token_id)
    decoder_input_ids = torch.full((1, 1), pad_token_id, dtype=torch.long, device=device)
    
    # ç”¨äºæ£€æµ‹é‡å¤çš„å˜é‡
    last_generated_id = -1
    repetition_count = 0
    REPETITION_THRESHOLD = 3 # å¦‚æœåŒä¸€ä¸ªIDè¿ç»­å‡ºç°3æ¬¡ï¼Œå°±åœæ­¢

    for _ in range(max_len - 1):
        logits = model.decoder(decoder_input_ids, encoder_output, source_padding_mask)
        next_token_logits = logits[:, -1, :]
        next_token_id = torch.argmax(next_token_logits, dim=-1).unsqueeze(1)
        
        # æ£€æŸ¥æ˜¯å¦ç”Ÿæˆäº†PAD token (è™½ç„¶å¯èƒ½æ€§å°ï¼Œä½†ä¿ç•™)
        if next_token_id.item() == pad_token_id:
            break
            
        # ã€æ ¸å¿ƒä¼˜åŒ–ã€‘æ£€æŸ¥å¹¶æ›´æ–°é‡å¤è®¡æ•°
        if next_token_id.item() == last_generated_id:
            repetition_count += 1
        else:
            repetition_count = 1 # é‡ç½®è®¡æ•°
        last_generated_id = next_token_id.item()
        
        # å¦‚æœè¾¾åˆ°é‡å¤é˜ˆå€¼ï¼Œåˆ™åœæ­¢
        if repetition_count >= REPETITION_THRESHOLD:
            # print(f" (Stopping due to {REPETITION_THRESHOLD} repetitions of token {last_generated_id})") # for debugging
            break

        decoder_input_ids = torch.cat([decoder_input_ids, next_token_id], dim=1)
            
    return decoder_input_ids.squeeze(0)[1:].tolist()


def main():
    parser = argparse.ArgumentParser(description="Visualize GENIUS-Rec model predictions.")
    parser.add_argument('--checkpoint_path', type=str, required=True,
                        help='Path to the model checkpoint file (e.g., checkpoints/checkpoints_genius_rec/genius_rec_best.pth).')
    parser.add_argument('--num_samples', type=int, default=5,
                        help='Number of samples to visualize from the validation set.')
    args = parser.parse_args()

    # --- 1. åŠ è½½é…ç½® ---
    print("--- 1. Loading Config ---")
    config = get_config()
    device = torch.device(config['device'])
    pad_token_id = config['pad_token_id']
    
    split_point = int(config['decoder_model']['max_seq_len'] * config['finetune']['split_ratio'])
    decoder_len = config['decoder_model']['max_seq_len'] - split_point

    # --- 2. åŠ è½½éªŒè¯æ•°æ®é›† ---
    print("--- 2. Loading Validation Dataset ---")
    with open(config['data']['id_maps_file'], 'rb') as f:
        id_maps = pickle.load(f)
        num_items = id_maps['num_items'] + 1

    val_dataset = Seq2SeqRecDataset(
        config['data']['validation_file'],
        config['decoder_model']['max_seq_len'],
        pad_token_id=pad_token_id,
        split_ratio=config['finetune']['split_ratio']
    )
    val_loader = DataLoader(val_dataset, batch_size=1, shuffle=True)
    print(f"Validation set loaded. Size: {len(val_dataset)}")

    # --- 3. åˆå§‹åŒ–æ¨¡å‹å¹¶åŠ è½½æ£€æŸ¥ç‚¹ ---
    print(f"--- 3. Loading Model from: {args.checkpoint_path} ---")
    encoder_config = {**config['encoder_model'], 'item_num': num_items, 'pad_token_id': pad_token_id}
    decoder_config = {**config['decoder_model'], 'num_items': num_items, 'pad_token_id': pad_token_id}
    model = GENIUSRecModel(encoder_config=encoder_config, decoder_config=decoder_config).to(device)
    
    try:
        checkpoint = torch.load(args.checkpoint_path, map_location=device)
        model.load_state_dict(checkpoint['model_state_dict'])
        print("Model state loaded successfully!")
    except FileNotFoundError:
        print(f"Error: Checkpoint file not found at {args.checkpoint_path}")
        return
    except Exception as e:
        print(f"An error occurred while loading the model: {e}")
        return

    # --- 4. è¿›è¡Œé¢„æµ‹å¹¶å¯è§†åŒ– ---
    print("\n" + "="*50)
    print(" " * 15 + "VISUALIZING PREDICTIONS")
    print("="*50 + "\n")

    with torch.no_grad():
        for i, batch in enumerate(val_loader):
            if i >= args.num_samples:
                break

            source_ids = batch['source_ids'].to(device)
            labels = batch['labels'].squeeze(0)

            predicted_ids = generate_sequence(
                model, 
                source_ids, 
                max_len=decoder_len, 
                pad_token_id=pad_token_id, 
                device=device
            )

            history_ids = [pid for pid in source_ids.squeeze(0).tolist() if pid != pad_token_id]
            actual_ids = [pid for pid in labels.tolist() if pid != pad_token_id]

            print(f"--- Sample #{i+1} ---")
            print(f"ğŸ‘¤ ç”¨æˆ·äº¤äº’å†å² (è¾“å…¥ç¼–ç å™¨éƒ¨åˆ†):\n   {history_ids}\n")
            print(f"ğŸ¤– æ¨¡å‹é¢„æµ‹ç»“æœ (ç”Ÿæˆçš„äº¤äº’åºåˆ—):\n   {predicted_ids}\n")
            print(f"âœ… å®é™…äº¤äº’å†å²:\n   {actual_ids}\n")
            print("-" * 25)

if __name__ == '__main__':
    main()